{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# Required imports for models and training\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to download the dataset from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reference: https://huggingface.co/datasets/rajpurkar/squad_v2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Reference: https://huggingface.co/datasets/rajpurkar/squad_v2'''\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"rajpurkar/squad_v2\")\n",
    "# ds.save_to_disk(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the presaved dataset\n",
    "from datasets import load_from_disk\n",
    "squad_v2 = load_from_disk(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n",
      "{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n",
      "19029\n"
     ]
    }
   ],
   "source": [
    "print(squad_v2)\n",
    "\n",
    "#printing the questions and answers for the same context\n",
    "train_dataset = squad_v2[\"train\"]\n",
    "val_dataset = squad_v2[\"validation\"]\n",
    "print(train_dataset[0])\n",
    "\n",
    "contexts = set()\n",
    "for i in range(len(train_dataset)):\n",
    "    contexts.add(train_dataset[i][\"context\"])\n",
    "print(len(contexts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def exact_match_score(predictions, references):\n",
    "    assert len(predictions) == len(references), \"Lists must have the same length\"\n",
    "    matches = sum(p == r for p, r in zip(predictions, references))\n",
    "    return matches / len(references) * 100  # Convert to percentage\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "def preprocess_data(examples, tokenizer, max_length=384):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = [c.strip() for c in examples[\"context\"]]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    tokenized = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Get the sample mapping\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    \n",
    "    # Get answer positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        # Get the sample index from the mapping\n",
    "        sample_idx = sample_mapping[i]\n",
    "        \n",
    "        # Get the answer for this sample\n",
    "        answer = examples[\"answers\"][sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0] if answer[\"text\"] else -1\n",
    "        end_char = start_char + len(answer[\"text\"][0]) if answer[\"text\"] else -1\n",
    "        \n",
    "        # Get sequence IDs to find context tokens\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        \n",
    "        # Find context boundaries\n",
    "        idx = 0\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        \n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "        \n",
    "        # Set answer positions\n",
    "        if start_char == -1 or end_char == -1:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and not (offset[idx][0] <= start_char < offset[idx][1]):\n",
    "                idx += 1\n",
    "            start_positions.append(idx if idx <= context_end else 0)\n",
    "            \n",
    "            idx = context_end\n",
    "            while idx >= context_start and not (offset[idx][0] <= end_char <= offset[idx][1]):\n",
    "                idx -= 1\n",
    "            end_positions.append(idx if idx >= context_start else 0)\n",
    "    \n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized\n",
    "\n",
    "# Model definitions\n",
    "class SpanBERTModel(nn.Module):\n",
    "    def __init__(self, model_name=\"SpanBERT/spanbert-base-cased\"):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, start_positions=None, end_positions=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "# Initialize models and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SpanBERT/spanbert-base-cased\")\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 2e-2,\n",
    "    \"num_epochs\": 10,\n",
    "    \"max_length\": 256,\n",
    "    \"train_size\": 15000  # As per requirement to use at least 15000 samples\n",
    "}\n",
    "\n",
    "# Prepare datasets with optimized processing\n",
    "print(\"Preprocessing training data...\")\n",
    "train_subset = train_dataset.select(range(config[\"train_size\"]))\n",
    "\n",
    "# Prepare datasets with smaller batch for testing\n",
    "print(\"Preprocessing training data...\")\n",
    "train_subset = train_dataset.select(range(config[\"train_size\"]))\n",
    "train_encoded = train_subset.map(\n",
    "    lambda x: preprocess_data(x, tokenizer, config[\"max_length\"]),\n",
    "    batched=True,\n",
    "    remove_columns=train_subset.column_names,\n",
    "    batch_size=32  # Process in smaller batches\n",
    ")\n",
    "\n",
    "print(\"Preprocessing validation data...\")\n",
    "val_encoded = val_dataset.map(\n",
    "    lambda x: preprocess_data(x, tokenizer, config[\"max_length\"]),\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    batch_size=32  # Process in smaller batches\n",
    ")\n",
    "\n",
    "# Save the encoded data\n",
    "save_dir = \"encoded_data\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "print(\"Saving encoded datasets...\")\n",
    "train_encoded.save_to_disk(os.path.join(save_dir, \"train_encoded\"))\n",
    "val_encoded.save_to_disk(os.path.join(save_dir, \"val_encoded\"))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SpanBERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1912/1912 [10:17<00:00,  3.09it/s, loss=6.1185]\n",
      "Validation: 100%|██████████| 1538/1538 [02:27<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Average training loss: 5.9808\n",
      "Average validation loss: 5.9506\n",
      "Exact match score: 51.46%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   6%|▌         | 116/1912 [00:37<10:00,  2.99it/s, loss=5.9353]"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encoded_dataset, max_length=384):\n",
    "        self.encoded_dataset = encoded_dataset\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encoded_dataset[idx]\n",
    "        \n",
    "        # Ensure all sequences are of the same length\n",
    "        input_ids = item['input_ids'][:self.max_length]\n",
    "        attention_mask = item['attention_mask'][:self.max_length]\n",
    "        \n",
    "        # Pad sequences if necessary\n",
    "        if len(input_ids) < self.max_length:\n",
    "            padding_length = self.max_length - len(input_ids)\n",
    "            input_ids = input_ids + [0] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "        \n",
    "        # Convert to tensors\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'start_positions': torch.tensor(item['start_positions'], dtype=torch.long),\n",
    "            'end_positions': torch.tensor(item['end_positions'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle variable length sequences\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    start_positions = torch.stack([item['start_positions'] for item in batch])\n",
    "    end_positions = torch.stack([item['end_positions'] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'start_positions': start_positions,\n",
    "        'end_positions': end_positions\n",
    "    }\n",
    "\n",
    "# Load encoded datasets\n",
    "save_dir = \"encoded_data\"\n",
    "train_encoded = load_from_disk(os.path.join(save_dir, \"train_encoded\"))\n",
    "val_encoded = load_from_disk(os.path.join(save_dir, \"val_encoded\"))\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = QADataset(train_encoded, max_length=config[\"max_length\"])\n",
    "val_dataset = QADataset(val_encoded, max_length=config[\"max_length\"])\n",
    "\n",
    "# Create dataloaders with custom collate function\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    em_scores = []\n",
    "    \n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    start_positions=batch[\"start_positions\"],\n",
    "                    end_positions=batch[\"end_positions\"]\n",
    "                )\n",
    "                \n",
    "                loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs.loss\n",
    "                total_train_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                try:\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    \n",
    "                    outputs = model(\n",
    "                        input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        start_positions=batch[\"start_positions\"],\n",
    "                        end_positions=batch[\"end_positions\"]\n",
    "                    )\n",
    "                    \n",
    "                    loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs.loss\n",
    "                    total_val_loss += loss.item()\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    start_logits = outputs.start_logits\n",
    "                    end_logits = outputs.end_logits\n",
    "                    \n",
    "                    start_pred = torch.argmax(start_logits, dim=1)\n",
    "                    end_pred = torch.argmax(end_logits, dim=1)\n",
    "                    \n",
    "                    # Convert predictions to text\n",
    "                    for i in range(len(start_pred)):\n",
    "                        pred_text = tokenizer.decode(\n",
    "                            batch[\"input_ids\"][i][start_pred[i]:end_pred[i]+1],\n",
    "                            skip_special_tokens=True\n",
    "                        )\n",
    "                        ref_text = tokenizer.decode(\n",
    "                            batch[\"input_ids\"][i][batch[\"start_positions\"][i]:batch[\"end_positions\"][i]+1],\n",
    "                            skip_special_tokens=True\n",
    "                        )\n",
    "                        predictions.append(pred_text)\n",
    "                        references.append(ref_text)\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Calculate exact match score\n",
    "        em_score = exact_match_score(predictions, references)\n",
    "        em_scores.append(em_score)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Exact match score: {em_score:.2f}%\")\n",
    "        print(\"-\" * 50) \n",
    "    \n",
    "    return train_losses, val_losses, em_scores\n",
    "# Train SpanBERT model\n",
    "print(\"Training SpanBERT model...\")\n",
    "spanbert_model = SpanBERTModel()\n",
    "train_losses, val_losses, em_scores = train_model(spanbert_model, train_dataloader, val_dataloader, config)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot EM scores\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(em_scores, label='EM Score', color='green')\n",
    "plt.title('Exact Match Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
