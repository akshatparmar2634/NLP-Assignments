{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Descriptions (First 5 Records):\n",
      "931874353976938497: people sitting on the floor in a large room with a wall\n",
      "880425829246922752: two twee screens of donald trump and donald trump\n",
      "690915881082343424: there are two shovels that are standing in the snow\n",
      "915228456757059585: arafed view of a passenger plane with a flat screen tv\n",
      "494194068998468686_25639236: cars are driving down the highway on a cloudy day\n",
      "\n",
      "Detected Objects (First 5 Records):\n",
      "931874353976938497: {'classes': ['person', 'backpack', 'handbag', 'backpack', 'backpack', 'cell phone', 'person', 'person', 'person', 'cup', 'chair', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'chair', 'person', 'person', 'person', 'backpack', 'backpack', 'person', 'person', 'person', 'backpack', 'person', 'person', 'person', 'person', 'person', 'person'], 'confidence_scores': [0.0945774, 0.0975185, 0.111666, 0.117207, 0.118647, 0.120145, 0.121325, 0.217946, 0.224531, 0.226831, 0.280922, 0.29603, 0.338583, 0.342417, 0.345206, 0.39319, 0.414227, 0.458103, 0.46252, 0.46994, 0.560848, 0.581421, 0.591951, 0.597088, 0.614235, 0.66626, 0.672012, 0.706699, 0.722024, 0.725346, 0.774559, 0.795291, 0.842849, 0.894795, 0.895111, 0.928385]}\n",
      "880425829246922752: {'classes': ['tv', 'book', 'person', 'person'], 'confidence_scores': [0.0566157, 0.0645416, 0.787489, 0.796628]}\n",
      "915228456757059585: {'classes': ['tv', 'chair', 'tv', 'chair', 'chair', 'person', 'tv', 'chair', 'chair', 'tv', 'tv', 'chair', 'chair', 'tv', 'person', 'chair', 'chair', 'person', 'tv', 'person', 'person', 'tv'], 'confidence_scores': [0.0644448, 0.0680951, 0.0796244, 0.101709, 0.134147, 0.140395, 0.154054, 0.190344, 0.214901, 0.244845, 0.312947, 0.48098, 0.596227, 0.775801, 0.79419, 0.829292, 0.831051, 0.836855, 0.849639, 0.865242, 0.867333, 0.916105]}\n",
      "494194068998468686_25639236: {'classes': ['car', 'car', 'car', 'car', 'car', 'truck', 'truck', 'car', 'truck', 'truck', 'car', 'car', 'truck', 'truck', 'car', 'car'], 'confidence_scores': [0.105599, 0.138541, 0.15192, 0.183974, 0.193817, 0.216392, 0.399908, 0.465357, 0.471086, 0.49458, 0.577627, 0.634711, 0.830119, 0.863202, 0.874461, 0.91342]}\n",
      "886345583052681217: {'classes': ['boat', 'boat', 'boat', 'boat', 'bench', 'boat', 'boat', 'bench', 'bottle', 'boat', 'boat', 'bench', 'boat', 'chair'], 'confidence_scores': [0.0647115, 0.0925675, 0.178609, 0.269199, 0.305418, 0.328585, 0.341485, 0.358296, 0.362327, 0.450375, 0.514758, 0.572053, 0.582355, 0.91542]}\n"
     ]
    }
   ],
   "source": [
    "# text data\n",
    "df = pd.read_csv(\"train_df.tsv\", sep=\"\\t\") \n",
    "\n",
    "# img descriptions\n",
    "with open(\"D_train.pkl\", \"rb\") as f:\n",
    "    image_descriptions = pickle.load(f)\n",
    "print(\"Image Descriptions (First 5 Records):\")\n",
    "for key, value in list(image_descriptions.items())[:5]:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# detected objects\n",
    "with open(\"O_train.pkl\", \"rb\") as f:\n",
    "    detected_objects = pickle.load(f)\n",
    "print(\"Detected Objects (First 5 Records):\")\n",
    "for key, value in list(detected_objects.items())[:5]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Download NLTK data for METEOR\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuSEDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text_file, image_desc_file, obj_file, image_folder, tokenizer, max_length=256, transform=None):\n",
    "        ''' Initialize the MuSEDataset class. '''\n",
    "        self.text_data = pd.read_csv(text_file, sep=\"\\t\")\n",
    "        \n",
    "        with open(image_desc_file, \"rb\") as f:\n",
    "            self.image_descriptions = pickle.load(f)\n",
    "        \n",
    "        with open(obj_file, \"rb\") as f:\n",
    "            self.detected_objects = pickle.load(f)\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "            # mean and std dev for RGB channels\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return the length of the dataset. '''\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the data for a given index.\n",
    "        Here, we get:\n",
    "        1. Multimodal input (txt, img desc, obj)\n",
    "        2. Explanantion and target\n",
    "        Tokenize them and return a dictionary of relevant data\n",
    "        '''        # Get data\n",
    "        row = self.text_data.iloc[idx]\n",
    "        text = row[\"text\"]  \n",
    "        explanation = row[\"explanation\"] if \"explanation\" in row else \"\"\n",
    "        image_name = str(row[\"pid\"])  # Convert to string to match dictionary keys\n",
    "        sarcasm_target = str(row[\"target_of_sarcasm\"]) if pd.notna(row.get(\"target_of_sarcasm\", \"\")) else \"\"\n",
    "\n",
    "        # Preprocess img\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_name}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            # blank image if not found\n",
    "            print(f\"Warning: Image {image_path} not found, using blank image.\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        # img desc and objects\n",
    "        img_desc = self.image_descriptions.get(image_name, \"No description available\")\n",
    "        detected_objs = self.detected_objects.get(image_name, {\"classes\": [], \"confidence_scores\": []})\n",
    "        \n",
    "        # formatting objs to string\n",
    "        if isinstance(detected_objs, dict) and \"classes\" in detected_objs:\n",
    "            obj_str = \", \".join(detected_objs[\"classes\"])\n",
    "        else:\n",
    "            obj_str = \"No objects detected\"\n",
    "\n",
    "        # multimodal input\n",
    "        multimodal_text = f\"Text: {text} Image: {img_desc} Objects: {obj_str}\"\n",
    "        \n",
    "        # Tokenize inputs, explanation, target\n",
    "        input_encodings = self.tokenizer(multimodal_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encodings = self.tokenizer(explanation, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        sarcasm_target_encodings = self.tokenizer(sarcasm_target, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # Return the data as a dictionary\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"image\": image,\n",
    "            \"target_ids\": target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"target_attention_mask\": target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"sarcasm_target_ids\": sarcasm_target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"sarcasm_target_mask\": sarcasm_target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"raw_text\": text,\n",
    "            \"raw_explanation\": explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProjection(nn.Module):\n",
    "    \"\"\"Projects ViT image features to match BART embedding dimensions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=768, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f\"Image Projection Input Shape: {x.shape}\")\n",
    "        return self.linear(x)\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Computes cross-modal attention between text and vision features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_v = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.query_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_t = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.scale = feature_dim ** 0.5\n",
    "        \n",
    "    def forward(self, vision_features, text_features):\n",
    "        # print(f\"Cross-Modal Input Shapes: Vision={vision_features.shape}, Text={text_features.shape}\")\n",
    "        # self-attention for vision \n",
    "        q_v = self.query_v(vision_features)\n",
    "        k_v = self.key_v(vision_features)\n",
    "        v_v = self.value_v(vision_features)\n",
    "        \n",
    "        attn_v = torch.matmul(q_v, k_v.transpose(-2, -1)) / self.scale\n",
    "        attn_v = F.softmax(attn_v, dim=-1)\n",
    "        attn_v = self.dropout(attn_v)\n",
    "        Av = torch.matmul(attn_v, v_v)\n",
    "        \n",
    "        # self-attention for text \n",
    "        q_t = self.query_t(text_features)\n",
    "        k_t = self.key_t(text_features)\n",
    "        v_t = self.value_t(text_features)\n",
    "        \n",
    "        attn_t = torch.matmul(q_t, k_t.transpose(-2, -1)) / self.scale\n",
    "        attn_t = F.softmax(attn_t, dim=-1)\n",
    "        attn_t = self.dropout(attn_t)\n",
    "        At = torch.matmul(attn_t, v_t)\n",
    "        \n",
    "        # cross modal\n",
    "        Ftv = At * vision_features\n",
    "        Fvt = Av * text_features\n",
    "        \n",
    "        return Ftv, Fvt\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Gated fusion mechanism to dynamically weigh features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wt = nn.Linear(feature_dim, feature_dim)\n",
    "        self.bv = nn.Parameter(torch.zeros(feature_dim))\n",
    "        self.bt = nn.Parameter(torch.zeros(feature_dim))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, Ev, Et, Ftv, Fvt):\n",
    "        # print(f\"Gated Fusion Input Shapes: Ev={Ev.shape}, Et={Et.shape}, Ftv={Ftv.shape}, Fvt={Fvt.shape}\")\n",
    "        # Gate for controlling information flow\n",
    "        Gv = torch.sigmoid(self.Wv(Ev) + self.bv)\n",
    "        Gt = torch.sigmoid(self.Wt(Et) + self.bt)\n",
    "        \n",
    "        # Multimodal features\n",
    "        F1 = (Gv * Ftv) + ((1 - Gv) * Fvt)\n",
    "        F2 = (Gt * Ftv) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        # Unimodal features\n",
    "        Fv = (Gv * Ev) + ((1 - Gv) * Ftv)\n",
    "        Ft = (Gt * Et) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        # Apply dropout\n",
    "        F1 = self.dropout(F1)\n",
    "        F2 = self.dropout(F2)\n",
    "        Fv = self.dropout(Fv)\n",
    "        Ft = self.dropout(Ft)\n",
    "\n",
    "        return F1, F2, Fv, Ft\n",
    "\n",
    "class SharedFusion(nn.Module):\n",
    "    \"\"\"Shared fusion mechanism that combines multimodal and unimodal features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.alpha1 = nn.Parameter(torch.ones(1))\n",
    "        self.alpha2 = nn.Parameter(torch.ones(1))\n",
    "        self.beta1 = nn.Parameter(torch.ones(1))\n",
    "        self.beta2 = nn.Parameter(torch.ones(1))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, F1, F2, Fv, Ft):\n",
    "        # print(f\"Shared Fusion Input Shapes: F1={F1.shape}, F2={F2.shape}, Fv={Fv.shape}, Ft={Ft.shape}\")\n",
    "        # Linear combination\n",
    "        FSF = self.alpha1 * F1 + self.alpha2 * F2 + self.beta1 * Fv + self.beta2 * Ft\n",
    "        FSF = self.dropout(FSF)\n",
    "        return FSF\n",
    "\n",
    "class MuSEModel(nn.Module):\n",
    "    \"\"\"Complete MuSE model for sarcasm explanation generation\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768, max_length=256):\n",
    "        super().__init__()\n",
    "        # Vision - ViT\n",
    "        self.vit = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        self.vit.heads = nn.Identity()  # Remove classification head\n",
    "        self.image_projection = ImageProjection(feature_dim, feature_dim)\n",
    "        \n",
    "        # Text - BART\n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_modal_attention = CrossModalAttention(feature_dim)\n",
    "        \n",
    "        # Fusion components\n",
    "        self.gated_fusion = GatedFusion(feature_dim)\n",
    "        self.shared_fusion = SharedFusion(feature_dim)\n",
    "        \n",
    "        # Dimensions and positioning\n",
    "        self.feature_dim = feature_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Target information projection \n",
    "        self.target_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        # Final projection (match BART dimensions)\n",
    "        self.output_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        print(\"Initializing MuSEModel...\")\n",
    "        \n",
    "    def embed_text(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract text embeddings using BART encoder\"\"\"\n",
    "        encoder_outputs = self.bart.get_encoder()(  # Direct encoder access\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return encoder_outputs.last_hidden_state\n",
    "    \n",
    "    def embed_image(self, images):\n",
    "        \"\"\"Extract image embeddings using ViT\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.vit(images)\n",
    "        return self.image_projection(image_features)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, images, target_ids=None, sarcasm_target_ids=None, sarcasm_target_mask=None):\n",
    "        '''\n",
    "        Et - text embeddings\n",
    "        Ev - vision embeddings\n",
    "        \n",
    "        Ftv, Fvt - cross modal attention(Et,Ev)\n",
    "        \n",
    "        F1, F2, Fv, Ft - gated fusion(Ev, Et, Ftv, Fvt)\n",
    "        FSF - shared fusion(F1, F2, Fv, Ft)\n",
    "        Z - final output\n",
    "        '''\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Et is embeddings for (text + target)\n",
    "        \n",
    "        Et = self.embed_text(input_ids, attention_mask)\n",
    "        if sarcasm_target_ids is not None and sarcasm_target_mask is not None:\n",
    "            target_embed = self.embed_text(sarcasm_target_ids, sarcasm_target_mask)\n",
    "            target_embed = self.target_projection(target_embed)\n",
    "            Et = Et + target_embed\n",
    "        \n",
    "        # Vision embeddings\n",
    "        Ev = self.embed_image(images)\n",
    "        Ev = Ev.unsqueeze(1).expand(-1, self.max_length, -1)\n",
    "        # print(f\"Expanded Vision Embeddings Shape: {Ev.shape}\")\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        Ftv, Fvt = self.cross_modal_attention(Ev, Et)\n",
    "        # print(f\"Cross-Modal Attention Output Shapes: Ftv={Ftv.shape}, Fvt={Fvt.shape}\")\n",
    "        \n",
    "        # Gated fusion\n",
    "        F1, F2, Fv, Ft = self.gated_fusion(Ev, Et, Ftv, Fvt)\n",
    "        # print(f\"Gated Fusion Output Shapes: F1={F1.shape}, F2={F2.shape}, Fv={Fv.shape}, Ft={Ft.shape}\")\n",
    "        \n",
    "        # Shared fusion\n",
    "        FSF = self.shared_fusion(F1, F2, Fv, Ft)\n",
    "        # print(f\"Shared Fusion Output Shape: {FSF.shape}\")\n",
    "        \n",
    "        # Final projection\n",
    "        Z = self.output_projection(FSF) + Et\n",
    "        # print(f\"Final Projection Output Shape: {Z.shape}\")\n",
    "        \n",
    "        if target_ids is not None:\n",
    "            target_length = target_ids.size(1)\n",
    "            Z = Z[:, :target_length, :]\n",
    "            outputs = self.bart(\n",
    "                encoder_outputs=(Z,),\n",
    "                attention_mask=attention_mask,\n",
    "                labels=target_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs\n",
    "        else:\n",
    "            generated_ids = self.bart.generate(\n",
    "                encoder_outputs=(Z,),\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"Total number of batches: {num_batches}\")\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        raise ValueError(\"Dataloader is empty!\")\n",
    "        \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            # Move data to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            if batch_idx % 20 == 0:  # Print every 10 batches\n",
    "                print(f\"Batch {batch_idx} loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            raise e\n",
    "            \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch complete. Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, tokenizer, device, num_samples=5):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    generated_explanations = []\n",
    "    reference_explanations = []\n",
    "    sample_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Move inputs to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass with teacher forcing for loss calculation\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            \n",
    "            # Get loss\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Get explanations\n",
    "            generated_ids = model.bart.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Decode explanations\n",
    "            for i in range(input_ids.size(0)):\n",
    "                gen_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(target_ids[i], skip_special_tokens=True)\n",
    "                \n",
    "                generated_explanations.append(gen_text)\n",
    "                reference_explanations.append(ref_text)\n",
    "                \n",
    "                # Save sample results for display\n",
    "                if len(sample_results) < num_samples:\n",
    "                    original_text = batch[\"raw_text\"][i]\n",
    "                    sample_results.append({\n",
    "                        \"text\": original_text,\n",
    "                        \"generated\": gen_text,\n",
    "                        \"reference\": ref_text\n",
    "                    })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # ROUGE\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=reference_explanations\n",
    "    )\n",
    "\n",
    "    # BLEU\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_results = bleu.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=[[r] for r in reference_explanations]  # Ensure references are in a list of lists\n",
    "    )\n",
    "\n",
    "    # METEOR (using NLTK)\n",
    "    from nltk.translate.meteor_score import meteor_score\n",
    "    meteor_scores = [meteor_score([ref], pred) for pred, ref in zip(generated_explanations, reference_explanations)]\n",
    "    meteor_results = {\"meteor\": sum(meteor_scores) / len(meteor_scores)}\n",
    "\n",
    "    # BERTScore\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bertscore_results = bertscore.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=reference_explanations,\n",
    "        lang=\"en\"  # Update if using a different language\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(\"ROUGE:\", rouge_results)\n",
    "    print(\"BLEU:\", bleu_results)\n",
    "    print(\"METEOR:\", meteor_results)\n",
    "    print(\"BERTScore:\", bertscore_results)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rouge\": {\n",
    "            \"rouge1\": rouge_results[\"rouge1\"],\n",
    "            \"rouge2\": rouge_results[\"rouge2\"],\n",
    "            \"rougeL\": rouge_results[\"rougeL\"]\n",
    "        },\n",
    "        \"bleu\": {\n",
    "            \"bleu1\": bleu_results[\"precisions\"][0],\n",
    "            \"bleu2\": bleu_results[\"precisions\"][1],\n",
    "            \"bleu3\": bleu_results[\"precisions\"][2],\n",
    "            \"bleu4\": bleu_results[\"precisions\"][3]\n",
    "        },\n",
    "        \"meteor\": meteor_results[\"meteor\"],\n",
    "        \"bertscore\": bertscore_results[\"f1\"][0],\n",
    "        \"samples\": sample_results[:5]\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tokenizer, device, \n",
    "                 epochs, lr, checkpoint_dir=\"checkpoints\"):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    bleu_scores = {\"bleu1\": [], \"bleu2\": [], \"bleu3\": [], \"bleu4\": []}\n",
    "    meteor_scores = []\n",
    "    bertscore_scores = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # train - 1 epoch\n",
    "        print(f\"Training on epoch {epoch + 1}...\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # validate\n",
    "        val_results = validate(model, val_loader, tokenizer, device)\n",
    "        val_losses.append(val_results[\"loss\"])\n",
    "        \n",
    "        # Store all metrics\n",
    "        for key in rouge_scores.keys():\n",
    "            rouge_scores[key].append(val_results[\"rouge\"][key])\n",
    "        for key in bleu_scores.keys():\n",
    "            bleu_scores[key].append(val_results[\"bleu\"][key])\n",
    "        meteor_scores.append(val_results[\"meteor\"])\n",
    "        bertscore_scores.append(val_results[\"bertscore\"])\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "        print(\"ROUGE Scores:\")\n",
    "        print(f\"  ROUGE-1: {val_results['rouge']['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2: {val_results['rouge']['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {val_results['rouge']['rougeL']:.4f}\")\n",
    "        print(\"BLEU Scores:\")\n",
    "        print(f\"  BLEU-1: {val_results['bleu']['bleu1']:.4f}\")\n",
    "        print(f\"  BLEU-2: {val_results['bleu']['bleu2']:.4f}\")\n",
    "        print(f\"  BLEU-3: {val_results['bleu']['bleu3']:.4f}\")\n",
    "        print(f\"  BLEU-4: {val_results['bleu']['bleu4']:.4f}\")\n",
    "        print(f\"METEOR: {val_results['meteor']:.4f}\")\n",
    "        print(f\"BERTScore: {val_results['bertscore']:.4f}\")\n",
    "        \n",
    "        # sample results\n",
    "        print(\"\\nSample Results:\")\n",
    "        for i, sample in enumerate(val_results[\"samples\"]):\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"Text: {sample['text']}\")\n",
    "            print(f\"Generated: {sample['generated']}\")\n",
    "            print(f\"Reference: {sample['reference']}\")\n",
    "            print()\n",
    "        \n",
    "        # save checkpt\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"muse_model_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_results[\"loss\"],\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    \n",
    "    # Plot 2: ROUGE scores\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(rouge_scores['rouge1'], label='ROUGE-1')\n",
    "    plt.plot(rouge_scores['rouge2'], label='ROUGE-2')\n",
    "    plt.plot(rouge_scores['rougeL'], label='ROUGE-L')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('ROUGE Scores')\n",
    "    \n",
    "    # Plot 3: BLEU scores\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(bleu_scores['bleu1'], label='BLEU-1')\n",
    "    plt.plot(bleu_scores['bleu2'], label='BLEU-2')\n",
    "    plt.plot(bleu_scores['bleu3'], label='BLEU-3')\n",
    "    plt.plot(bleu_scores['bleu4'], label='BLEU-4')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('BLEU Scores')\n",
    "    \n",
    "    # Plot 4: METEOR score\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(meteor_scores, label='METEOR')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('METEOR Score')\n",
    "    \n",
    "    # Plot 5: BERTScore\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(bertscore_scores, label='BERTScore')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('BERTScore')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'training_progress.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'rouge_scores': rouge_scores,\n",
    "        'bleu_scores': bleu_scores,\n",
    "        'meteor_scores': meteor_scores,\n",
    "        'bertscore_scores': bertscore_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def train_and_validate():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # BART tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    from torch.utils.data import Subset\n",
    "\n",
    "    # Full training dataset\n",
    "    train_dataset = MuSEDataset(\n",
    "        text_file=\"train_df.tsv\", \n",
    "        image_desc_file=\"D_train.pkl\", \n",
    "        obj_file=\"O_train.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Create a subset of the first 100 samples for training\n",
    "    train_indices = list(range(100))\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # dataloaders\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Number of batches: {len(train_loader)}\")\n",
    "        \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(len(val_loader.dataset))\n",
    "\n",
    "        # model\n",
    "    model = MuSEModel().to(device)\n",
    "    \n",
    "    # train\n",
    "    train_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        epochs=20,\n",
    "        lr=1e-5,\n",
    "        checkpoint_dir=\"checkpoints\"\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    print(\"Saving final model...\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = os.path.join(\"checkpoints\", \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model, train_results\n",
    "\n",
    "def test_model(model_path=\"checkpoints/final_model.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Load model\n",
    "    model = MuSEModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test data\n",
    "    test_dataset = MuSEDataset(\n",
    "        text_file=\"test_df.tsv\", \n",
    "        image_desc_file=\"D_test.pkl\", \n",
    "        obj_file=\"O_test.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_results = validate(model, test_loader, tokenizer, device)\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                max_length=50\n",
    "            )\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    with open(\"test_predictions.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(pred + \"\\n\")\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_results['loss']:.4f}\")\n",
    "    print(f\"ROUGE-1: {test_results['rouge']['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {test_results['rouge']['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {test_results['rouge']['rougeL']:.4f}\")\n",
    "    print(f\"BLEU: {test_results['bleu']['bleu']:.4f}\")\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train dataset size: 2684\n",
      "Number of batches: 168\n",
      "299\n",
      "Initializing MuSEModel...\n",
      "Epoch 1/20\n",
      "Training on epoch 1...\n",
      "Total number of batches: 168\n",
      "Batch 0 loss: 16.5991\n"
     ]
    }
   ],
   "source": [
    "model, train_results = train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing MuSEModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Local\\Temp\\ipykernel_26092\\1689813157.py:91: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_df.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 95\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMuSEDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_df.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_desc_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD_test.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mO_test.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m    104\u001b[0m     test_dataset, \n\u001b[0;32m    105\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[0;32m    106\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m    107\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mMuSEDataset.__init__\u001b[1;34m(self, text_file, image_desc_file, obj_file, image_folder, tokenizer, max_length, transform)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_file, image_desc_file, obj_file, image_folder, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Initialize the MuSEDataset class. '''\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(image_desc_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_descriptions \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_df.tsv'"
     ]
    }
   ],
   "source": [
    "test_results = test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
