{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text data\n",
    "df = pd.read_csv(\"train_df.tsv\", sep=\"\\t\") \n",
    "\n",
    "# img descriptions\n",
    "with open(\"D_train.pkl\", \"rb\") as f:\n",
    "    image_descriptions = pickle.load(f)\n",
    "print(\"Image Descriptions (First 5 Records):\")\n",
    "for key, value in list(image_descriptions.items())[:5]:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# detected objects\n",
    "with open(\"O_train.pkl\", \"rb\") as f:\n",
    "    detected_objects = pickle.load(f)\n",
    "print(\"Detected Objects (First 5 Records):\")\n",
    "for key, value in list(detected_objects.items())[:5]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Download NLTK data for METEOR\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuSEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text_file, image_desc_file, obj_file, image_folder, tokenizer, max_length=256, transform=None):\n",
    "        ''' Initialize the MuSEDataset class. '''\n",
    "        self.text_data = pd.read_csv(text_file, sep=\"\\t\")\n",
    "\n",
    "        with open(image_desc_file, \"rb\") as f:\n",
    "            self.image_descriptions = pickle.load(f)\n",
    "\n",
    "        with open(obj_file, \"rb\") as f:\n",
    "            self.detected_objects = pickle.load(f)\n",
    "\n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "            # mean and std dev for RGB channels\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        ''' Return the length of the dataset. '''\n",
    "        return len(self.text_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the data for a given index.\n",
    "        Here, we get:\n",
    "        1. Multimodal input (txt, img desc, obj)\n",
    "        2. Explanantion and target\n",
    "        Tokenize them and return a dictionary of relevant data\n",
    "        '''        # Get data\n",
    "        row = self.text_data.iloc[idx]\n",
    "        text = row[\"text\"]\n",
    "        explanation = row[\"explanation\"] if \"explanation\" in row else \"\"\n",
    "        image_name = str(row[\"pid\"])  # Convert to string to match dictionary keys\n",
    "        sarcasm_target = str(row[\"target_of_sarcasm\"]) if pd.notna(row.get(\"target_of_sarcasm\", \"\")) else \"\"\n",
    "\n",
    "        # Preprocess img\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_name}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            # blank image if not found\n",
    "            print(f\"Warning: Image {image_path} not found, using blank image.\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        # img desc and objects\n",
    "        img_desc = self.image_descriptions.get(image_name, \"No description available\")\n",
    "        detected_objs = self.detected_objects.get(image_name, {\"classes\": [], \"confidence_scores\": []})\n",
    "\n",
    "        # formatting objs to string\n",
    "        if isinstance(detected_objs, dict) and \"classes\" in detected_objs:\n",
    "            obj_str = \", \".join(detected_objs[\"classes\"])\n",
    "        else:\n",
    "            obj_str = \"No objects detected\"\n",
    "\n",
    "        # multimodal input\n",
    "        multimodal_text = f\"Text: {text} Image: {img_desc} Objects: {obj_str}\"\n",
    "\n",
    "        # Tokenize inputs, explanation, target\n",
    "        input_encodings = self.tokenizer(multimodal_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encodings = self.tokenizer(explanation, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        sarcasm_target_encodings = self.tokenizer(sarcasm_target, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # Return the data as a dictionary\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"image\": image,\n",
    "            \"target_ids\": target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"target_attention_mask\": target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"sarcasm_target_ids\": sarcasm_target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"sarcasm_target_mask\": sarcasm_target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"raw_text\": text,\n",
    "            \"raw_explanation\": explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProjection(nn.Module):\n",
    "    \"\"\"Projects ViT image features to match BART embedding dimensions\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=768, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Image Projection Input Shape: {x.shape}\")\n",
    "        return self.linear(x)\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Computes cross-modal attention between text and vision features\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_v = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "        self.query_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_t = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.scale = feature_dim ** 0.5\n",
    "\n",
    "    def forward(self, vision_features, text_features):\n",
    "        # print(f\"Cross-Modal Input Shapes: Vision={vision_features.shape}, Text={text_features.shape}\")\n",
    "        # self-attention for vision\n",
    "        q_v = self.query_v(vision_features)\n",
    "        k_v = self.key_v(vision_features)\n",
    "        v_v = self.value_v(vision_features)\n",
    "\n",
    "        attn_v = torch.matmul(q_v, k_v.transpose(-2, -1)) / self.scale\n",
    "        attn_v = F.softmax(attn_v, dim=-1)\n",
    "        attn_v = self.dropout(attn_v)\n",
    "        Av = torch.matmul(attn_v, v_v)\n",
    "\n",
    "        # self-attention for text\n",
    "        q_t = self.query_t(text_features)\n",
    "        k_t = self.key_t(text_features)\n",
    "        v_t = self.value_t(text_features)\n",
    "\n",
    "        attn_t = torch.matmul(q_t, k_t.transpose(-2, -1)) / self.scale\n",
    "        attn_t = F.softmax(attn_t, dim=-1)\n",
    "        attn_t = self.dropout(attn_t)\n",
    "        At = torch.matmul(attn_t, v_t)\n",
    "\n",
    "        # cross modal\n",
    "        Ftv = At * vision_features\n",
    "        Fvt = Av * text_features\n",
    "\n",
    "        return Ftv, Fvt\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Gated fusion mechanism to dynamically weigh features\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wt = nn.Linear(feature_dim, feature_dim)\n",
    "        self.bv = nn.Parameter(torch.zeros(feature_dim))\n",
    "        self.bt = nn.Parameter(torch.zeros(feature_dim))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, Ev, Et, Ftv, Fvt):\n",
    "        # print(f\"Gated Fusion Input Shapes: Ev={Ev.shape}, Et={Et.shape}, Ftv={Ftv.shape}, Fvt={Fvt.shape}\")\n",
    "        # Gate for controlling information flow\n",
    "        Gv = torch.sigmoid(self.Wv(Ev) + self.bv)\n",
    "        Gt = torch.sigmoid(self.Wt(Et) + self.bt)\n",
    "\n",
    "        # Multimodal features\n",
    "        F1 = (Gv * Ftv) + ((1 - Gv) * Fvt)\n",
    "        F2 = (Gt * Ftv) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        # Unimodal features\n",
    "        Fv = (Gv * Ev) + ((1 - Gv) * Ftv)\n",
    "        Ft = (Gt * Et) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        # Apply dropout\n",
    "        F1 = self.dropout(F1)\n",
    "        F2 = self.dropout(F2)\n",
    "        Fv = self.dropout(Fv)\n",
    "        Ft = self.dropout(Ft)\n",
    "\n",
    "        return F1, F2, Fv, Ft\n",
    "\n",
    "class SharedFusion(nn.Module):\n",
    "    \"\"\"Shared fusion mechanism that combines multimodal and unimodal features\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.alpha1 = nn.Parameter(torch.ones(1))\n",
    "        self.alpha2 = nn.Parameter(torch.ones(1))\n",
    "        self.beta1 = nn.Parameter(torch.ones(1))\n",
    "        self.beta2 = nn.Parameter(torch.ones(1))\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, F1, F2, Fv, Ft):\n",
    "        # print(f\"Shared Fusion Input Shapes: F1={F1.shape}, F2={F2.shape}, Fv={Fv.shape}, Ft={Ft.shape}\")\n",
    "        # Linear combination\n",
    "        FSF = self.alpha1 * F1 + self.alpha2 * F2 + self.beta1 * Fv + self.beta2 * Ft\n",
    "        FSF = self.dropout(FSF)\n",
    "        return FSF\n",
    "\n",
    "class MuSEModel(nn.Module):\n",
    "    \"\"\"Complete MuSE model for sarcasm explanation generation\"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=768, max_length=256):\n",
    "        super().__init__()\n",
    "        # Vision - ViT\n",
    "        self.vit = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        self.vit.heads = nn.Identity()  # Remove classification head\n",
    "        self.image_projection = ImageProjection(feature_dim, feature_dim)\n",
    "\n",
    "        # Text - BART\n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "        # Cross-modal attention\n",
    "        self.cross_modal_attention = CrossModalAttention(feature_dim)\n",
    "\n",
    "        # Fusion components\n",
    "        self.gated_fusion = GatedFusion(feature_dim)\n",
    "        self.shared_fusion = SharedFusion(feature_dim)\n",
    "\n",
    "        # Dimensions and positioning\n",
    "        self.feature_dim = feature_dim\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Target information projection\n",
    "        self.target_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        # Final projection (match BART dimensions)\n",
    "        self.output_projection = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "        print(\"Initializing MuSEModel...\")\n",
    "\n",
    "    def embed_text(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract text embeddings using BART encoder\"\"\"\n",
    "        encoder_outputs = self.bart.get_encoder()(  # Direct encoder access\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return encoder_outputs.last_hidden_state\n",
    "\n",
    "    def embed_image(self, images):\n",
    "        \"\"\"Extract image embeddings using ViT\"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.vit(images)\n",
    "        return self.image_projection(image_features)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images, target_ids=None, sarcasm_target_ids=None, sarcasm_target_mask=None):\n",
    "      batch_size = input_ids.size(0)\n",
    "\n",
    "      # Et is embeddings for (text + target)\n",
    "      Et = self.embed_text(input_ids, attention_mask)\n",
    "      if sarcasm_target_ids is not None and sarcasm_target_mask is not None:\n",
    "          target_embed = self.embed_text(sarcasm_target_ids, sarcasm_target_mask)\n",
    "          target_embed = self.target_projection(target_embed)\n",
    "          Et = Et + target_embed\n",
    "\n",
    "      # Vision embeddings\n",
    "      Ev = self.embed_image(images)\n",
    "      Ev = Ev.unsqueeze(1).expand(-1, self.max_length, -1)\n",
    "\n",
    "      # Cross-modal attention\n",
    "      Ftv, Fvt = self.cross_modal_attention(Ev, Et)\n",
    "\n",
    "      # Gated fusion\n",
    "      F1, F2, Fv, Ft = self.gated_fusion(Ev, Et, Ftv, Fvt)\n",
    "\n",
    "      # Shared fusion\n",
    "      FSF = self.shared_fusion(F1, F2, Fv, Ft)\n",
    "\n",
    "      # Final projection\n",
    "      Z = self.output_projection(FSF)\n",
    "\n",
    "      if target_ids is not None:\n",
    "          # Training mode - compute loss\n",
    "          target_length = target_ids.size(1)\n",
    "          Z = Z[:, :target_length, :]\n",
    "          outputs = self.bart(\n",
    "              encoder_outputs=(Z,),\n",
    "              attention_mask=attention_mask,\n",
    "              labels=target_ids,\n",
    "              return_dict=True\n",
    "          )\n",
    "          return outputs\n",
    "      else:\n",
    "          # Inference mode - generate text\n",
    "          # Pass Z (shared fusion embeddings) to the BART encoder\n",
    "          encoder_outputs = self.bart.model.encoder(\n",
    "              inputs_embeds=Z,  # Use Z as input embeddings\n",
    "              attention_mask=attention_mask,\n",
    "              return_dict=True\n",
    "          )\n",
    "\n",
    "          # Greedy decoding\n",
    "          decoder_input_ids = torch.full(\n",
    "              (batch_size, 1),\n",
    "              self.bart.config.decoder_start_token_id,\n",
    "              dtype=torch.long\n",
    "          ).to(input_ids.device)\n",
    "          generated_ids = decoder_input_ids\n",
    "\n",
    "          # Decoding loop\n",
    "          for _ in range(self.max_length-1): \n",
    "              decoder_outputs = self.bart.model.decoder(\n",
    "                  input_ids=generated_ids,\n",
    "                  encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "                  encoder_attention_mask=attention_mask,\n",
    "                  return_dict=True\n",
    "              )\n",
    "\n",
    "              # Get the next token (greedy decoding)\n",
    "              next_token_logits = decoder_outputs.logits[:, -1, :]\n",
    "              next_token_ids = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "              # Append the next token to the generated sequence\n",
    "              generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n",
    "\n",
    "          return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"Total number of batches: {num_batches}\")\n",
    "\n",
    "    if num_batches == 0:\n",
    "        raise ValueError(\"Dataloader is empty!\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            # Move data to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            if batch_idx % 20 == 0:  # Print every 10 batches\n",
    "                print(f\"Batch {batch_idx} loss: {loss.item():.4f}\")\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch complete. Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, tokenizer, device, num_samples=5):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    generated_explanations = []\n",
    "    reference_explanations = []\n",
    "    sample_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Move inputs to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "\n",
    "            # Forward pass with loss\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # For teacher-forced validation, model returns logits\n",
    "            # Convert logits to token predictions\n",
    "            logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "            # Decode generated and reference texts\n",
    "            for i in range(input_ids.size(0)):\n",
    "                gen_text = tokenizer.decode(predicted_ids[i], skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(target_ids[i], skip_special_tokens=True)\n",
    "\n",
    "                generated_explanations.append(gen_text)\n",
    "                reference_explanations.append(ref_text)\n",
    "\n",
    "                if len(sample_results) < num_samples:\n",
    "                    original_text = batch[\"raw_text\"][i]\n",
    "                    sample_results.append({\n",
    "                        \"text\": original_text,\n",
    "                        \"generated\": gen_text,\n",
    "                        \"reference\": ref_text\n",
    "                    })\n",
    "\n",
    "    # Calculate metrics\n",
    "    import evaluate\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=reference_explanations\n",
    "    )\n",
    "\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_results = bleu.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=[[r] for r in reference_explanations]\n",
    "    )\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    meteor_results = [\n",
    "        meteor_score([word_tokenize(ref)], word_tokenize(pred))\n",
    "        for pred, ref in zip(generated_explanations, reference_explanations)\n",
    "    ]\n",
    "\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bertscore_results = bertscore.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=reference_explanations,\n",
    "        lang=\"en\"\n",
    "    )\n",
    "\n",
    "    print(\"ROUGE:\", rouge_results)\n",
    "    print(\"BLEU:\", bleu_results)\n",
    "    print(\"METEOR:\", meteor_results)\n",
    "    print(\"BERTScore:\", bertscore_results)\n",
    "\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rouge\": {\n",
    "            \"rouge1\": rouge_results[\"rouge1\"],\n",
    "            \"rouge2\": rouge_results[\"rouge2\"],\n",
    "            \"rougeL\": rouge_results[\"rougeL\"]\n",
    "        },\n",
    "        \"bleu\": {\n",
    "            \"bleu1\": bleu_results[\"precisions\"][0],\n",
    "            \"bleu2\": bleu_results[\"precisions\"][1],\n",
    "            \"bleu3\": bleu_results[\"precisions\"][2],\n",
    "            \"bleu4\": bleu_results[\"precisions\"][3]\n",
    "        },\n",
    "        \"meteor\": meteor_results[\"meteor\"],\n",
    "        \"bertscore\": bertscore_results[\"f1\"][0],\n",
    "        \"samples\": sample_results[:5]\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tokenizer, device,\n",
    "                 epochs, lr, checkpoint_dir=\"checkpoints\"):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    bleu_scores = {\"bleu1\": [], \"bleu2\": [], \"bleu3\": [], \"bleu4\": []}\n",
    "    meteor_scores = []\n",
    "    bertscore_scores = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        # train - 1 epoch\n",
    "        print(f\"Training on epoch {epoch + 1}...\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # validate\n",
    "        val_results = validate(model, val_loader, tokenizer, device)\n",
    "        val_losses.append(val_results[\"loss\"])\n",
    "\n",
    "        # Store all metrics\n",
    "        for key in rouge_scores.keys():\n",
    "            rouge_scores[key].append(val_results[\"rouge\"][key])\n",
    "        for key in bleu_scores.keys():\n",
    "            bleu_scores[key].append(val_results[\"bleu\"][key])\n",
    "        meteor_scores.append(val_results[\"meteor\"])\n",
    "        bertscore_scores.append(val_results[\"bertscore\"])\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "        print(\"ROUGE Scores:\")\n",
    "        print(f\"  ROUGE-1: {val_results['rouge']['rouge1']:.4f}\")\n",
    "        print(f\"  ROUGE-2: {val_results['rouge']['rouge2']:.4f}\")\n",
    "        print(f\"  ROUGE-L: {val_results['rouge']['rougeL']:.4f}\")\n",
    "        print(\"BLEU Scores:\")\n",
    "        print(f\"  BLEU-1: {val_results['bleu']['bleu1']:.4f}\")\n",
    "        print(f\"  BLEU-2: {val_results['bleu']['bleu2']:.4f}\")\n",
    "        print(f\"  BLEU-3: {val_results['bleu']['bleu3']:.4f}\")\n",
    "        print(f\"  BLEU-4: {val_results['bleu']['bleu4']:.4f}\")\n",
    "        print(f\"METEOR: {val_results['meteor']:.4f}\")\n",
    "        print(f\"BERTScore: {val_results['bertscore']:.4f}\")\n",
    "\n",
    "        # sample results\n",
    "        print(\"\\nSample Results:\")\n",
    "        for i, sample in enumerate(val_results[\"samples\"]):\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"Text: {sample['text']}\")\n",
    "            print(f\"Generated: {sample['generated']}\")\n",
    "            print(f\"Reference: {sample['reference']}\")\n",
    "            print()\n",
    "\n",
    "        # save checkpt\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"muse_model_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_results[\"loss\"],\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # Plot 1: Loss curves\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    # Plot 2: ROUGE scores\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(rouge_scores['rouge1'], label='ROUGE-1')\n",
    "    plt.plot(rouge_scores['rouge2'], label='ROUGE-2')\n",
    "    plt.plot(rouge_scores['rougeL'], label='ROUGE-L')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('ROUGE Scores')\n",
    "\n",
    "    # Plot 3: BLEU scores\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(bleu_scores['bleu1'], label='BLEU-1')\n",
    "    plt.plot(bleu_scores['bleu2'], label='BLEU-2')\n",
    "    plt.plot(bleu_scores['bleu3'], label='BLEU-3')\n",
    "    plt.plot(bleu_scores['bleu4'], label='BLEU-4')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('BLEU Scores')\n",
    "\n",
    "    # Plot 4: METEOR score\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(meteor_scores, label='METEOR')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('METEOR Score')\n",
    "\n",
    "    # Plot 5: BERTScore\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(bertscore_scores, label='BERTScore')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.title('BERTScore')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'training_progress.png'))\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'rouge_scores': rouge_scores,\n",
    "        'bleu_scores': bleu_scores,\n",
    "        'meteor_scores': meteor_scores,\n",
    "        'bertscore_scores': bertscore_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def train_and_validate():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # BART tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "\n",
    "    # Full training dataset\n",
    "    train_dataset = MuSEDataset(\n",
    "        text_file=\"train_df.tsv\",\n",
    "        image_desc_file=\"D_train.pkl\",\n",
    "        obj_file=\"O_train.pkl\",\n",
    "        image_folder=\"images\",\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Number of batches: {len(train_loader)}\")\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print(len(val_loader.dataset))\n",
    "\n",
    "        # model\n",
    "    model = MuSEModel().to(device)\n",
    "\n",
    "    # train\n",
    "    train_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        epochs=20,\n",
    "        lr=1e-5,\n",
    "        checkpoint_dir=\"checkpoints\"\n",
    "    )\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "    print(\"Saving final model...\")\n",
    "\n",
    "    # Save final model\n",
    "    model_path = os.path.join(\"checkpoints\", \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    return model, train_results\n",
    "\n",
    "def test_model(model_path=\"checkpoints/final_model.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "    # Load model\n",
    "    model = MuSEModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Load test data\n",
    "    test_dataset = MuSEDataset(\n",
    "        text_file=\"test_df.tsv\",\n",
    "        image_desc_file=\"D_test.pkl\",\n",
    "        obj_file=\"O_test.pkl\",\n",
    "        image_folder=\"images\",\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    test_results = validate(model, test_loader, tokenizer, device)\n",
    "\n",
    "    # Save predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                max_length=50\n",
    "            )\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    with open(\"test_predictions.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(pred + \"\\n\")\n",
    "\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_results['loss']:.4f}\")\n",
    "    print(f\"ROUGE-1: {test_results['rouge']['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {test_results['rouge']['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {test_results['rouge']['rougeL']:.4f}\")\n",
    "    print(f\"BLEU: {test_results['bleu']['bleu']:.4f}\")\n",
    "\n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_results = train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
