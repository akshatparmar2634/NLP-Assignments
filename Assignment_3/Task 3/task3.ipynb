{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Descriptions (First 5 Records):\n",
      "931874353976938497: people sitting on the floor in a large room with a wall\n",
      "880425829246922752: two twee screens of donald trump and donald trump\n",
      "690915881082343424: there are two shovels that are standing in the snow\n",
      "915228456757059585: arafed view of a passenger plane with a flat screen tv\n",
      "494194068998468686_25639236: cars are driving down the highway on a cloudy day\n",
      "\n",
      "Detected Objects (First 5 Records):\n",
      "931874353976938497: {'classes': ['person', 'backpack', 'handbag', 'backpack', 'backpack', 'cell phone', 'person', 'person', 'person', 'cup', 'chair', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'chair', 'person', 'person', 'person', 'backpack', 'backpack', 'person', 'person', 'person', 'backpack', 'person', 'person', 'person', 'person', 'person', 'person'], 'confidence_scores': [0.0945774, 0.0975185, 0.111666, 0.117207, 0.118647, 0.120145, 0.121325, 0.217946, 0.224531, 0.226831, 0.280922, 0.29603, 0.338583, 0.342417, 0.345206, 0.39319, 0.414227, 0.458103, 0.46252, 0.46994, 0.560848, 0.581421, 0.591951, 0.597088, 0.614235, 0.66626, 0.672012, 0.706699, 0.722024, 0.725346, 0.774559, 0.795291, 0.842849, 0.894795, 0.895111, 0.928385]}\n",
      "880425829246922752: {'classes': ['tv', 'book', 'person', 'person'], 'confidence_scores': [0.0566157, 0.0645416, 0.787489, 0.796628]}\n",
      "915228456757059585: {'classes': ['tv', 'chair', 'tv', 'chair', 'chair', 'person', 'tv', 'chair', 'chair', 'tv', 'tv', 'chair', 'chair', 'tv', 'person', 'chair', 'chair', 'person', 'tv', 'person', 'person', 'tv'], 'confidence_scores': [0.0644448, 0.0680951, 0.0796244, 0.101709, 0.134147, 0.140395, 0.154054, 0.190344, 0.214901, 0.244845, 0.312947, 0.48098, 0.596227, 0.775801, 0.79419, 0.829292, 0.831051, 0.836855, 0.849639, 0.865242, 0.867333, 0.916105]}\n",
      "494194068998468686_25639236: {'classes': ['car', 'car', 'car', 'car', 'car', 'truck', 'truck', 'car', 'truck', 'truck', 'car', 'car', 'truck', 'truck', 'car', 'car'], 'confidence_scores': [0.105599, 0.138541, 0.15192, 0.183974, 0.193817, 0.216392, 0.399908, 0.465357, 0.471086, 0.49458, 0.577627, 0.634711, 0.830119, 0.863202, 0.874461, 0.91342]}\n",
      "886345583052681217: {'classes': ['boat', 'boat', 'boat', 'boat', 'bench', 'boat', 'boat', 'bench', 'bottle', 'boat', 'boat', 'bench', 'boat', 'chair'], 'confidence_scores': [0.0647115, 0.0925675, 0.178609, 0.269199, 0.305418, 0.328585, 0.341485, 0.358296, 0.362327, 0.450375, 0.514758, 0.572053, 0.582355, 0.91542]}\n"
     ]
    }
   ],
   "source": [
    "# text data\n",
    "df = pd.read_csv(\"train_df.tsv\", sep=\"\\t\")\n",
    "\n",
    "# img descriptions\n",
    "with open(\"D_train.pkl\", \"rb\") as f:\n",
    "    image_descriptions = pickle.load(f)\n",
    "print(\"Image Descriptions (First 5 Records):\")\n",
    "for key, value in list(image_descriptions.items())[:5]:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# detected objects\n",
    "with open(\"O_train.pkl\", \"rb\") as f:\n",
    "    detected_objects = pickle.load(f)\n",
    "print(\"Detected Objects (First 5 Records):\")\n",
    "for key, value in list(detected_objects.items())[:5]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuSEDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text_file, image_desc_file, obj_file, image_folder, tokenizer, max_length=256, transform=None):\n",
    "        ''' Initialize the MuSEDataset class. '''\n",
    "        self.text_data = pd.read_csv(text_file, sep=\"\\t\")\n",
    "        \n",
    "        with open(image_desc_file, \"rb\") as f:\n",
    "            self.image_descriptions = pickle.load(f)\n",
    "        \n",
    "        with open(obj_file, \"rb\") as f:\n",
    "            self.detected_objects = pickle.load(f)\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "            # mean and std dev for RGB channels\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return the length of the dataset. '''\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the data for a given index.\n",
    "        Here, we get:\n",
    "        1. Multimodal input (txt, img desc, obj)\n",
    "        2. Explanantion and target\n",
    "        Tokenize them and return a dictionary of relevant data\n",
    "        '''\n",
    "        # Get data\n",
    "        row = self.text_data.iloc[idx]\n",
    "        text = row[\"text\"]  \n",
    "        explanation = row[\"explanation\"] if \"explanation\" in row else \"\"\n",
    "        image_name = str(row[\"pid\"])  # Convert to string to match dictionary keys\n",
    "        sarcasm_target = str(row[\"target_of_sarcasm\"]) if pd.notna(row.get(\"target_of_sarcasm\", \"\")) else \"\"\n",
    "\n",
    "        # Preprocess img\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_name}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            # blank image if not found\n",
    "            print(f\"Warning: Image {image_path} not found, using blank image.\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        # img desc and objects\n",
    "        img_desc = self.image_descriptions.get(image_name, \"No description available\")\n",
    "        detected_objs = self.detected_objects.get(image_name, {\"classes\": [], \"confidence_scores\": []})\n",
    "        \n",
    "        # formatting objs to string\n",
    "        if isinstance(detected_objs, dict) and \"classes\" in detected_objs:\n",
    "            obj_str = \", \".join(detected_objs[\"classes\"])\n",
    "        else:\n",
    "            obj_str = \"No objects detected\"\n",
    "\n",
    "        # multimodal input\n",
    "        multimodal_text = f\"Text: {text} Image: {img_desc} Objects: {obj_str}\"\n",
    "        \n",
    "        # Tokenize inputs, explanation, target\n",
    "        input_encodings = self.tokenizer(multimodal_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encodings = self.tokenizer(explanation, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        sarcasm_target_encodings = self.tokenizer(sarcasm_target, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"image\": image,\n",
    "            \"target_ids\": target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"target_attention_mask\": target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"sarcasm_target_ids\": sarcasm_target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"sarcasm_target_mask\": sarcasm_target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"raw_text\": text,\n",
    "            \"raw_explanation\": explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProjection(nn.Module):\n",
    "    \"\"\"Projects ViT image features to match BART embedding dimensions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=768, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Computes cross-modal attention between text and vision features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_v = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.query_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_t = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.scale = feature_dim ** 0.5\n",
    "        \n",
    "    def forward(self, vision_features, text_features):\n",
    "        # self-attention for vision \n",
    "        q_v = self.query_v(vision_features)\n",
    "        k_v = self.key_v(vision_features)\n",
    "        v_v = self.value_v(vision_features)\n",
    "        \n",
    "        attn_v = torch.matmul(q_v, k_v.transpose(-2, -1)) / self.scale\n",
    "        attn_v = F.softmax(attn_v, dim=-1)\n",
    "        attn_v = self.dropout(attn_v)\n",
    "        Av = torch.matmul(attn_v, v_v)\n",
    "        \n",
    "        # self-attention for text \n",
    "        q_t = self.query_t(text_features)\n",
    "        k_t = self.key_t(text_features)\n",
    "        v_t = self.value_t(text_features)\n",
    "        \n",
    "        attn_t = torch.matmul(q_t, k_t.transpose(-2, -1)) / self.scale\n",
    "        attn_t = F.softmax(attn_t, dim=-1)\n",
    "        attn_t = self.dropout(attn_t)\n",
    "        At = torch.matmul(attn_t, v_t)\n",
    "        \n",
    "        # cross modal\n",
    "        Ftv = At * vision_features\n",
    "        Fvt = Av * text_features\n",
    "        \n",
    "        return Ftv, Fvt\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Gated fusion mechanism to dynamically weigh features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wt = nn.Linear(feature_dim, feature_dim)\n",
    "        self.bv = nn.Parameter(torch.zeros(feature_dim))\n",
    "        self.bt = nn.Parameter(torch.zeros(feature_dim))\n",
    "    \n",
    "    def forward(self, Ev, Et, Ftv, Fvt):\n",
    "        # Gate for controlling information flow\n",
    "        Gv = torch.sigmoid(self.Wv(Ev) + self.bv)\n",
    "        Gt = torch.sigmoid(self.Wt(Et) + self.bt)\n",
    "        \n",
    "        # Multimodal features\n",
    "        F1 = (Gv * Ftv) + ((1 - Gv) * Fvt)\n",
    "        F2 = (Gt * Ftv) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        # Unimodal features\n",
    "        Fv = (Gv * Ev) + ((1 - Gv) * Ftv)\n",
    "        Ft = (Gt * Et) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        return F1, F2, Fv, Ft\n",
    "\n",
    "class SharedFusion(nn.Module):\n",
    "    \"\"\"Shared fusion mechanism that combines multimodal and unimodal features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.alpha1 = nn.Parameter(torch.ones(1))\n",
    "        self.alpha2 = nn.Parameter(torch.ones(1))\n",
    "        self.beta1 = nn.Parameter(torch.ones(1))\n",
    "        self.beta2 = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, F1, F2, Fv, Ft):\n",
    "        # Linear combination\n",
    "        FSF = self.alpha1 * F1 + self.alpha2 * F2 + self.beta1 * Fv + self.beta2 * Ft\n",
    "        return FSF\n",
    "\n",
    "class MuSEModel(nn.Module):\n",
    "    \"\"\"Complete MuSE model for sarcasm explanation generation\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768, max_length=256):\n",
    "        super().__init__()\n",
    "        # Vision - ViT\n",
    "        self.vit = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        self.vit.heads = nn.Identity()  # Remove classification head\n",
    "        self.image_projection = ImageProjection(feature_dim, feature_dim)\n",
    "        \n",
    "        # Text - BART\n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_modal_attention = CrossModalAttention(feature_dim)\n",
    "        \n",
    "        # Fusion components\n",
    "        self.gated_fusion = GatedFusion(feature_dim)\n",
    "        self.shared_fusion = SharedFusion(feature_dim)\n",
    "        \n",
    "        # Dimensions and positioning\n",
    "        self.feature_dim = feature_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Target information projection \n",
    "        self.target_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        # Final projection (match BART dimensions)\n",
    "        self.output_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "    def embed_text(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract text embeddings using BART encoder\"\"\"\n",
    "        \n",
    "        encoder_outputs = self.bart.model.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return encoder_outputs.last_hidden_state\n",
    "    \n",
    "    def embed_image(self, images):\n",
    "        \"\"\"Extract image embeddings using ViT\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.vit(images)\n",
    "        return self.image_projection(image_features)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, images, target_ids=None, sarcasm_target_ids=None, sarcasm_target_mask=None):\n",
    "        '''\n",
    "        Et - text embeddings\n",
    "        Ev - vision embeddings\n",
    "        \n",
    "        Ftv, Fvt - cross modal attention(Et,Ev)\n",
    "        \n",
    "        F1, F2, Fv, Ft - gated fusion(Ev, Et, Ftv, Fvt)\n",
    "        FSF - shared fusion(F1, F2, Fv, Ft)\n",
    "        Z - final output\n",
    "        '''\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Et is embeddings for (text + target)\n",
    "        Et = self.embed_text(input_ids, attention_mask)\n",
    "        if sarcasm_target_ids is not None and sarcasm_target_mask is not None:\n",
    "            target_embed = self.embed_text(sarcasm_target_ids, sarcasm_target_mask)\n",
    "            target_embed = self.target_projection(target_embed)\n",
    "            Et = Et + target_embed\n",
    "        \n",
    "        # Ev - vision embeddings -> reshape to match text embeddings size\n",
    "        Ev = self.embed_image(images)\n",
    "        Ev = Ev.unsqueeze(1).expand(-1, self.max_length, -1)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        Ftv, Fvt = self.cross_modal_attention(Ev, Et)\n",
    "        \n",
    "        # Gated fusion\n",
    "        F1, F2, Fv, Ft = self.gated_fusion(Ev, Et, Ftv, Fvt)\n",
    "        \n",
    "        # Shared fusion\n",
    "        FSF = self.shared_fusion(F1, F2, Fv, Ft)\n",
    "        \n",
    "        # Final proj\n",
    "        Z = self.output_projection(FSF) + Et  # Residual connection with text\n",
    "        \n",
    "        # Use BART decoder to generate the explanation\n",
    "        if target_ids is not None:\n",
    "            # Training mode - calculate loss\n",
    "            outputs = self.bart(\n",
    "                encoder_outputs=torch.utils.data.dataloader._utils.pin_memory.PinnedObject({\n",
    "                    'last_hidden_state': Z,\n",
    "                    'hidden_states': None,\n",
    "                    'attentions': None\n",
    "                }),\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=target_ids[:, :-1].contiguous() if target_ids.size(1) > 1 else None,\n",
    "                labels=target_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs\n",
    "        else:\n",
    "            # Inference mode - generate text\n",
    "            generated_ids = self.bart.generate(\n",
    "                encoder_outputs=torch.utils.data.dataloader._utils.pin_memory.PinnedObject({\n",
    "                    'last_hidden_state': Z,\n",
    "                    'hidden_states': None,\n",
    "                    'attentions': None\n",
    "                }),\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval import evaluate\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        # Move inputs to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        images = batch[\"image\"].to(device)\n",
    "        target_ids = batch[\"target_ids\"].to(device)\n",
    "        sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "        sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            images=images,\n",
    "            target_ids=target_ids,\n",
    "            sarcasm_target_ids=sarcasm_target_ids,\n",
    "            sarcasm_target_mask=sarcasm_target_mask\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Batch {idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, dataloader, tokenizer, device, num_samples=5):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    generated_explanations = []\n",
    "    reference_explanations = []\n",
    "    sample_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Move inputs to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass with teacher forcing for loss calculation\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            \n",
    "            # Get loss\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            #  get explanations\n",
    "            generated_ids = model.bart.generate(\n",
    "                encoder_outputs=torch.utils.data.dataloader._utils.pin_memory.PinnedObject({\n",
    "                    'last_hidden_state': model.embed_text(input_ids, attention_mask),\n",
    "                    'hidden_states': None,\n",
    "                    'attentions': None\n",
    "                }),\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # decode explanations\n",
    "            for i in range(input_ids.size(0)):\n",
    "                gen_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(target_ids[i], skip_special_tokens=True)\n",
    "                \n",
    "                generated_explanations.append(gen_text)\n",
    "                reference_explanations.append(ref_text)\n",
    "                \n",
    "                # Save sample results for display\n",
    "                if len(sample_results) < num_samples:\n",
    "                    original_text = batch[\"raw_text\"][i]\n",
    "                    sample_results.append({\n",
    "                        \"text\": original_text,\n",
    "                        \"generated\": gen_text,\n",
    "                        \"reference\": ref_text\n",
    "                    })\n",
    "    \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=reference_explanations\n",
    "    )\n",
    "    \n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_results = bleu.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=[[r] for r in reference_explanations]\n",
    "    )\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rouge\": rouge_results,\n",
    "        \"bleu\": bleu_results,\n",
    "        \"samples\": sample_results\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tokenizer, device, \n",
    "                 epochs=20, lr=0.001, checkpoint_dir=\"checkpoints\"):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    rouge_scores = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # train - 1 epoch\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # validate\n",
    "        val_results = validate(model, val_loader, tokenizer, device)\n",
    "        val_losses.append(val_results[\"loss\"])\n",
    "        rouge_scores.append(val_results[\"rouge\"][\"rougeL\"])\n",
    "        bleu_scores.append(val_results[\"bleu\"][\"bleu\"])\n",
    "        \n",
    "        print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "        print(f\"ROUGE-L: {val_results['rouge']['rougeL']:.4f}\")\n",
    "        print(f\"BLEU: {val_results['bleu']['bleu']:.4f}\")\n",
    "        \n",
    "        # sample results\n",
    "        print(\"\\nSample Results:\")\n",
    "        for i, sample in enumerate(val_results[\"samples\"]):\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"Text: {sample['text']}\")\n",
    "            print(f\"Generated: {sample['generated']}\")\n",
    "            print(f\"Reference: {sample['reference']}\")\n",
    "            print()\n",
    "        \n",
    "        # save checkpt\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"muse_model_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_results[\"loss\"],\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(rouge_scores)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROUGE-L')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(bleu_scores)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BLEU')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'training_progress.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'rouge_scores': rouge_scores,\n",
    "        'bleu_scores': bleu_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "def train_and_validate():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # BART tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # datasets\n",
    "    train_dataset = MuSEDataset(\n",
    "        text_file=\"train_df.tsv\", \n",
    "        image_desc_file=\"D_train.pkl\", \n",
    "        obj_file=\"O_train.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # train and val sets (90-10 split)\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # dataloaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # model\n",
    "    model = MuSEModel().to(device)\n",
    "    \n",
    "    # train\n",
    "    train_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        epochs=5,\n",
    "        lr=5e-5,\n",
    "        checkpoint_dir=\"checkpoints\"\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = os.path.join(\"checkpoints\", \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model, train_results\n",
    "\n",
    "def test_model(model_path=\"checkpoints/final_model.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Load model\n",
    "    model = MuSEModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test data\n",
    "    test_dataset = MuSEDataset(\n",
    "        text_file=\"test_df.tsv\", \n",
    "        image_desc_file=\"D_test.pkl\", \n",
    "        obj_file=\"O_test.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_results = validate(model, test_loader, tokenizer, device)\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                max_length=50\n",
    "            )\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    with open(\"test_predictions.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(pred + \"\\n\")\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_results['loss']:.4f}\")\n",
    "    print(f\"ROUGE-1: {test_results['rouge']['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {test_results['rouge']['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {test_results['rouge']['rougeL']:.4f}\")\n",
    "    print(f\"BLEU: {test_results['bleu']['bleu']:.4f}\")\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_results = train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
