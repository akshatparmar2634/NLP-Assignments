{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Descriptions (First 5 Records):\n",
      "931874353976938497: people sitting on the floor in a large room with a wall\n",
      "880425829246922752: two twee screens of donald trump and donald trump\n",
      "690915881082343424: there are two shovels that are standing in the snow\n",
      "915228456757059585: arafed view of a passenger plane with a flat screen tv\n",
      "494194068998468686_25639236: cars are driving down the highway on a cloudy day\n",
      "\n",
      "Detected Objects (First 5 Records):\n",
      "931874353976938497: {'classes': ['person', 'backpack', 'handbag', 'backpack', 'backpack', 'cell phone', 'person', 'person', 'person', 'cup', 'chair', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'chair', 'person', 'person', 'person', 'backpack', 'backpack', 'person', 'person', 'person', 'backpack', 'person', 'person', 'person', 'person', 'person', 'person'], 'confidence_scores': [0.0945774, 0.0975185, 0.111666, 0.117207, 0.118647, 0.120145, 0.121325, 0.217946, 0.224531, 0.226831, 0.280922, 0.29603, 0.338583, 0.342417, 0.345206, 0.39319, 0.414227, 0.458103, 0.46252, 0.46994, 0.560848, 0.581421, 0.591951, 0.597088, 0.614235, 0.66626, 0.672012, 0.706699, 0.722024, 0.725346, 0.774559, 0.795291, 0.842849, 0.894795, 0.895111, 0.928385]}\n",
      "880425829246922752: {'classes': ['tv', 'book', 'person', 'person'], 'confidence_scores': [0.0566157, 0.0645416, 0.787489, 0.796628]}\n",
      "915228456757059585: {'classes': ['tv', 'chair', 'tv', 'chair', 'chair', 'person', 'tv', 'chair', 'chair', 'tv', 'tv', 'chair', 'chair', 'tv', 'person', 'chair', 'chair', 'person', 'tv', 'person', 'person', 'tv'], 'confidence_scores': [0.0644448, 0.0680951, 0.0796244, 0.101709, 0.134147, 0.140395, 0.154054, 0.190344, 0.214901, 0.244845, 0.312947, 0.48098, 0.596227, 0.775801, 0.79419, 0.829292, 0.831051, 0.836855, 0.849639, 0.865242, 0.867333, 0.916105]}\n",
      "494194068998468686_25639236: {'classes': ['car', 'car', 'car', 'car', 'car', 'truck', 'truck', 'car', 'truck', 'truck', 'car', 'car', 'truck', 'truck', 'car', 'car'], 'confidence_scores': [0.105599, 0.138541, 0.15192, 0.183974, 0.193817, 0.216392, 0.399908, 0.465357, 0.471086, 0.49458, 0.577627, 0.634711, 0.830119, 0.863202, 0.874461, 0.91342]}\n",
      "886345583052681217: {'classes': ['boat', 'boat', 'boat', 'boat', 'bench', 'boat', 'boat', 'bench', 'bottle', 'boat', 'boat', 'bench', 'boat', 'chair'], 'confidence_scores': [0.0647115, 0.0925675, 0.178609, 0.269199, 0.305418, 0.328585, 0.341485, 0.358296, 0.362327, 0.450375, 0.514758, 0.572053, 0.582355, 0.91542]}\n"
     ]
    }
   ],
   "source": [
    "# text data\n",
    "df = pd.read_csv(\"train_df.tsv\", sep=\"\\t\") \n",
    "\n",
    "# img descriptions\n",
    "with open(\"D_train.pkl\", \"rb\") as f:\n",
    "    image_descriptions = pickle.load(f)\n",
    "print(\"Image Descriptions (First 5 Records):\")\n",
    "for key, value in list(image_descriptions.items())[:5]:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# detected objects\n",
    "with open(\"O_train.pkl\", \"rb\") as f:\n",
    "    detected_objects = pickle.load(f)\n",
    "print(\"Detected Objects (First 5 Records):\")\n",
    "for key, value in list(detected_objects.items())[:5]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuSEDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text_file, image_desc_file, obj_file, image_folder, tokenizer, max_length=256, transform=None):\n",
    "        ''' Initialize the MuSEDataset class. '''\n",
    "        self.text_data = pd.read_csv(text_file, sep=\"\\t\")\n",
    "        \n",
    "        with open(image_desc_file, \"rb\") as f:\n",
    "            self.image_descriptions = pickle.load(f)\n",
    "        \n",
    "        with open(obj_file, \"rb\") as f:\n",
    "            self.detected_objects = pickle.load(f)\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "            # mean and std dev for RGB channels\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return the length of the dataset. '''\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the data for a given index.\n",
    "        Here, we get:\n",
    "        1. Multimodal input (txt, img desc, obj)\n",
    "        2. Explanantion and target\n",
    "        Tokenize them and return a dictionary of relevant data\n",
    "        '''        # Get data\n",
    "        row = self.text_data.iloc[idx]\n",
    "        text = row[\"text\"]  \n",
    "        explanation = row[\"explanation\"] if \"explanation\" in row else \"\"\n",
    "        image_name = str(row[\"pid\"])  # Convert to string to match dictionary keys\n",
    "        sarcasm_target = str(row[\"target_of_sarcasm\"]) if pd.notna(row.get(\"target_of_sarcasm\", \"\")) else \"\"\n",
    "\n",
    "        # Preprocess img\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_name}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            # blank image if not found\n",
    "            print(f\"Warning: Image {image_path} not found, using blank image.\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        # img desc and objects\n",
    "        img_desc = self.image_descriptions.get(image_name, \"No description available\")\n",
    "        detected_objs = self.detected_objects.get(image_name, {\"classes\": [], \"confidence_scores\": []})\n",
    "        \n",
    "        # formatting objs to string\n",
    "        if isinstance(detected_objs, dict) and \"classes\" in detected_objs:\n",
    "            obj_str = \", \".join(detected_objs[\"classes\"])\n",
    "        else:\n",
    "            obj_str = \"No objects detected\"\n",
    "\n",
    "        # multimodal input\n",
    "        multimodal_text = f\"Text: {text} Image: {img_desc} Objects: {obj_str}\"\n",
    "        \n",
    "        # Tokenize inputs, explanation, target\n",
    "        input_encodings = self.tokenizer(multimodal_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encodings = self.tokenizer(explanation, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        sarcasm_target_encodings = self.tokenizer(sarcasm_target, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # Return the data as a dictionary\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"image\": image,\n",
    "            \"target_ids\": target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"target_attention_mask\": target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"sarcasm_target_ids\": sarcasm_target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"sarcasm_target_mask\": sarcasm_target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"raw_text\": text,\n",
    "            \"raw_explanation\": explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProjection(nn.Module):\n",
    "    \"\"\"Projects ViT image features to match BART embedding dimensions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=768, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Computes cross-modal attention between text and vision features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_v = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.query_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_t = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.scale = feature_dim ** 0.5\n",
    "        \n",
    "    def forward(self, vision_features, text_features):\n",
    "        # self-attention for vision \n",
    "        q_v = self.query_v(vision_features)\n",
    "        k_v = self.key_v(vision_features)\n",
    "        v_v = self.value_v(vision_features)\n",
    "        \n",
    "        attn_v = torch.matmul(q_v, k_v.transpose(-2, -1)) / self.scale\n",
    "        attn_v = F.softmax(attn_v, dim=-1)\n",
    "        attn_v = self.dropout(attn_v)\n",
    "        Av = torch.matmul(attn_v, v_v)\n",
    "        \n",
    "        # self-attention for text \n",
    "        q_t = self.query_t(text_features)\n",
    "        k_t = self.key_t(text_features)\n",
    "        v_t = self.value_t(text_features)\n",
    "        \n",
    "        attn_t = torch.matmul(q_t, k_t.transpose(-2, -1)) / self.scale\n",
    "        attn_t = F.softmax(attn_t, dim=-1)\n",
    "        attn_t = self.dropout(attn_t)\n",
    "        At = torch.matmul(attn_t, v_t)\n",
    "        \n",
    "        # cross modal\n",
    "        Ftv = At * vision_features\n",
    "        Fvt = Av * text_features\n",
    "        \n",
    "        return Ftv, Fvt\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Gated fusion mechanism to dynamically weigh features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wt = nn.Linear(feature_dim, feature_dim)\n",
    "        self.bv = nn.Parameter(torch.zeros(feature_dim))\n",
    "        self.bt = nn.Parameter(torch.zeros(feature_dim))\n",
    "    \n",
    "    def forward(self, Ev, Et, Ftv, Fvt):\n",
    "        # Gate for controlling information flow\n",
    "        Gv = torch.sigmoid(self.Wv(Ev) + self.bv)\n",
    "        Gt = torch.sigmoid(self.Wt(Et) + self.bt)\n",
    "        \n",
    "        # Multimodal features\n",
    "        F1 = (Gv * Ftv) + ((1 - Gv) * Fvt)\n",
    "        F2 = (Gt * Ftv) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        # Unimodal features\n",
    "        Fv = (Gv * Ev) + ((1 - Gv) * Ftv)\n",
    "        Ft = (Gt * Et) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        return F1, F2, Fv, Ft\n",
    "\n",
    "class SharedFusion(nn.Module):\n",
    "    \"\"\"Shared fusion mechanism that combines multimodal and unimodal features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.alpha1 = nn.Parameter(torch.ones(1))\n",
    "        self.alpha2 = nn.Parameter(torch.ones(1))\n",
    "        self.beta1 = nn.Parameter(torch.ones(1))\n",
    "        self.beta2 = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, F1, F2, Fv, Ft):\n",
    "        # Linear combination\n",
    "        FSF = self.alpha1 * F1 + self.alpha2 * F2 + self.beta1 * Fv + self.beta2 * Ft\n",
    "        return FSF\n",
    "\n",
    "class MuSEModel(nn.Module):\n",
    "    \"\"\"Complete MuSE model for sarcasm explanation generation\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768, max_length=256):\n",
    "        super().__init__()\n",
    "        # Vision - ViT\n",
    "        self.vit = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        self.vit.heads = nn.Identity()  # Remove classification head\n",
    "        self.image_projection = ImageProjection(feature_dim, feature_dim)\n",
    "        \n",
    "        # Text - BART\n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_modal_attention = CrossModalAttention(feature_dim)\n",
    "        \n",
    "        # Fusion components\n",
    "        self.gated_fusion = GatedFusion(feature_dim)\n",
    "        self.shared_fusion = SharedFusion(feature_dim)\n",
    "        \n",
    "        # Dimensions and positioning\n",
    "        self.feature_dim = feature_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Target information projection \n",
    "        self.target_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        # Final projection (match BART dimensions)\n",
    "        self.output_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        print(\"Initializing MuSEModel...\")\n",
    "        \n",
    "    def embed_text(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract text embeddings using BART encoder\"\"\"\n",
    "        encoder_outputs = self.bart.get_encoder()(  # Direct encoder access\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return encoder_outputs.last_hidden_state\n",
    "    \n",
    "    def embed_image(self, images):\n",
    "        \"\"\"Extract image embeddings using ViT\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.vit(images)\n",
    "        return self.image_projection(image_features)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, images, target_ids=None, sarcasm_target_ids=None, sarcasm_target_mask=None):\n",
    "        '''\n",
    "        Et - text embeddings\n",
    "        Ev - vision embeddings\n",
    "        \n",
    "        Ftv, Fvt - cross modal attention(Et,Ev)\n",
    "        \n",
    "        F1, F2, Fv, Ft - gated fusion(Ev, Et, Ftv, Fvt)\n",
    "        FSF - shared fusion(F1, F2, Fv, Ft)\n",
    "        Z - final output\n",
    "        '''\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Et is embeddings for (text + target)\n",
    "        \n",
    "        Et = self.embed_text(input_ids, attention_mask)\n",
    "        if sarcasm_target_ids is not None and sarcasm_target_mask is not None:\n",
    "            target_embed = self.embed_text(sarcasm_target_ids, sarcasm_target_mask)\n",
    "            target_embed = self.target_projection(target_embed)\n",
    "            Et = Et + target_embed\n",
    "        \n",
    "        # Ev - vision embeddings -> reshape to match text embeddings size   \n",
    "        Ev = self.embed_image(images)\n",
    "        Ev = Ev.unsqueeze(1).expand(-1, self.max_length, -1)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        Ftv, Fvt = self.cross_modal_attention(Ev, Et)\n",
    "        \n",
    "        # Gated fusion\n",
    "        F1, F2, Fv, Ft = self.gated_fusion(Ev, Et, Ftv, Fvt)\n",
    "        \n",
    "        # Shared fusion\n",
    "        FSF = self.shared_fusion(F1, F2, Fv, Ft)\n",
    "        \n",
    "        # Final proj\n",
    "        Z = self.output_projection(FSF) + Et \n",
    "    \n",
    "        if target_ids is not None:\n",
    "            \n",
    "            target_length = target_ids.size(1)\n",
    "            Z = Z[:, :target_length, :]\n",
    "            \n",
    "            # Pass to BART with proper dimensions\n",
    "            outputs = self.bart(\n",
    "                encoder_outputs=(Z,),\n",
    "                attention_mask=attention_mask,\n",
    "                labels=target_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs\n",
    "        else:\n",
    "            # Inference mode - generate text\n",
    "            generated_ids = self.bart.generate(\n",
    "                encoder_outputs=(Z,),  # Proper tuple format\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "       \n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"Total number of batches: {num_batches}\")\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        raise ValueError(\"Dataloader is empty!\")\n",
    "        \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            \n",
    "            # Move data to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            print(f\"Batch {batch_idx} loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            raise e\n",
    "            \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch complete. Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, tokenizer, device, num_samples=5):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    generated_explanations = []\n",
    "    reference_explanations = []\n",
    "    sample_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Move inputs to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass with teacher forcing for loss calculation\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            \n",
    "            # Get loss\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            #  get explanations\n",
    "            generated_ids = model.bart.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # decode explanations\n",
    "            for i in range(input_ids.size(0)):\n",
    "                gen_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(target_ids[i], skip_special_tokens=True)\n",
    "                \n",
    "                generated_explanations.append(gen_text)\n",
    "                reference_explanations.append(ref_text)\n",
    "                \n",
    "                # Save sample results for display\n",
    "                if len(sample_results) < num_samples:\n",
    "                    original_text = batch[\"raw_text\"][i]\n",
    "                    sample_results.append({\n",
    "                        \"text\": original_text,\n",
    "                        \"generated\": gen_text,\n",
    "                        \"reference\": ref_text\n",
    "                    })\n",
    "    \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=reference_explanations\n",
    "    )\n",
    "    \n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_results = bleu.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=[[r] for r in reference_explanations]\n",
    "    )\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rouge\": rouge_results,\n",
    "        \"bleu\": bleu_results,\n",
    "        \"samples\": sample_results\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tokenizer, device, \n",
    "                 epochs=20, lr=0.001, checkpoint_dir=\"checkpoints\"):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    rouge_scores = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # train - 1 epoch\n",
    "        print(f\"Training on epoch {epoch + 1}...\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # validate\n",
    "        val_results = validate(model, val_loader, tokenizer, device)\n",
    "        val_losses.append(val_results[\"loss\"])\n",
    "        rouge_scores.append(val_results[\"rouge\"][\"rougeL\"])\n",
    "        bleu_scores.append(val_results[\"bleu\"][\"bleu\"])\n",
    "        \n",
    "        print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "        print(f\"ROUGE-L: {val_results['rouge']['rougeL']:.4f}\")\n",
    "        print(f\"BLEU: {val_results['bleu']['bleu']:.4f}\")\n",
    "        \n",
    "        # sample results\n",
    "        print(\"\\nSample Results:\")\n",
    "        for i, sample in enumerate(val_results[\"samples\"]):\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"Text: {sample['text']}\")\n",
    "            print(f\"Generated: {sample['generated']}\")\n",
    "            print(f\"Reference: {sample['reference']}\")\n",
    "            print()\n",
    "        \n",
    "        # save checkpt\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"muse_model_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_results[\"loss\"],\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(rouge_scores)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROUGE-L')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(bleu_scores)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BLEU')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'training_progress.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'rouge_scores': rouge_scores,\n",
    "        'bleu_scores': bleu_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # BART tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    from torch.utils.data import Subset\n",
    "\n",
    "    # Full training dataset\n",
    "    train_dataset = MuSEDataset(\n",
    "        text_file=\"train_df.tsv\", \n",
    "        image_desc_file=\"D_train.pkl\", \n",
    "        obj_file=\"O_train.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Create a subset of the first 100 samples for training\n",
    "    train_indices = list(range(100))\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # dataloaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Number of batches: {len(train_loader)}\")\n",
    "        \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(len(val_loader.dataset))\n",
    "\n",
    "        # model\n",
    "    model = MuSEModel().to(device)\n",
    "    \n",
    "    # train\n",
    "    train_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        epochs=5,\n",
    "        lr=5e-5,\n",
    "        checkpoint_dir=\"checkpoints\"\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    print(\"Saving final model...\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = os.path.join(\"checkpoints\", \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model, train_results\n",
    "\n",
    "def test_model(model_path=\"checkpoints/final_model.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Load model\n",
    "    model = MuSEModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test data\n",
    "    test_dataset = MuSEDataset(\n",
    "        text_file=\"test_df.tsv\", \n",
    "        image_desc_file=\"D_test.pkl\", \n",
    "        obj_file=\"O_test.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_results = validate(model, test_loader, tokenizer, device)\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                max_length=50\n",
    "            )\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    with open(\"test_predictions.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(pred + \"\\n\")\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_results['loss']:.4f}\")\n",
    "    print(f\"ROUGE-1: {test_results['rouge']['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {test_results['rouge']['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {test_results['rouge']['rougeL']:.4f}\")\n",
    "    print(f\"BLEU: {test_results['bleu']['bleu']:.4f}\")\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Train dataset size: 2684\n",
      "Number of batches: 168\n",
      "299\n",
      "Initializing MuSEModel...\n",
      "Epoch 1/5\n",
      "Training on epoch 1...\n",
      "Total number of batches: 168\n",
      "Batch 0 loss: 16.3001\n",
      "Batch 1 loss: 14.1793\n",
      "Batch 2 loss: 13.1673\n",
      "Batch 3 loss: 12.6746\n",
      "Batch 4 loss: 12.3869\n",
      "Batch 5 loss: 12.0750\n",
      "Batch 6 loss: 11.8671\n",
      "Batch 7 loss: 11.6205\n",
      "Batch 8 loss: 11.3489\n",
      "Batch 9 loss: 11.0736\n",
      "Batch 10 loss: 10.8328\n",
      "Batch 11 loss: 10.6870\n",
      "Batch 12 loss: 10.3064\n",
      "Batch 13 loss: 10.0501\n",
      "Batch 14 loss: 9.6351\n",
      "Batch 15 loss: 9.3908\n",
      "Batch 16 loss: 8.9434\n",
      "Batch 17 loss: 8.5678\n",
      "Batch 18 loss: 8.0828\n",
      "Batch 19 loss: 7.6576\n",
      "Batch 20 loss: 7.4528\n",
      "Batch 21 loss: 6.8204\n",
      "Batch 22 loss: 6.6538\n",
      "Batch 23 loss: 6.2778\n",
      "Batch 24 loss: 5.8965\n",
      "Batch 25 loss: 5.7938\n",
      "Batch 26 loss: 5.4649\n",
      "Batch 27 loss: 5.2924\n",
      "Batch 28 loss: 4.8621\n",
      "Batch 29 loss: 4.6792\n",
      "Batch 30 loss: 4.6325\n",
      "Batch 31 loss: 4.3114\n",
      "Batch 32 loss: 4.1710\n",
      "Batch 33 loss: 3.9977\n",
      "Batch 34 loss: 3.8566\n",
      "Batch 35 loss: 3.7152\n",
      "Batch 36 loss: 3.5602\n",
      "Batch 37 loss: 3.3773\n",
      "Batch 38 loss: 3.2381\n",
      "Batch 39 loss: 3.0227\n",
      "Batch 40 loss: 2.8992\n",
      "Batch 41 loss: 2.8413\n",
      "Batch 42 loss: 2.7042\n",
      "Batch 43 loss: 2.5416\n",
      "Batch 44 loss: 2.4627\n",
      "Batch 45 loss: 2.2292\n",
      "Batch 46 loss: 2.1203\n",
      "Batch 47 loss: 2.0441\n",
      "Batch 48 loss: 1.9107\n",
      "Batch 49 loss: 1.8739\n",
      "Batch 50 loss: 1.7325\n",
      "Batch 51 loss: 1.5932\n",
      "Batch 52 loss: 1.4659\n",
      "Batch 53 loss: 1.4903\n",
      "Batch 54 loss: 1.3094\n",
      "Batch 55 loss: 1.2610\n",
      "Batch 56 loss: 1.1560\n",
      "Batch 57 loss: 1.1340\n",
      "Batch 58 loss: 1.0451\n",
      "Batch 59 loss: 0.9511\n",
      "Batch 60 loss: 0.9126\n",
      "Batch 61 loss: 0.8874\n",
      "Batch 62 loss: 0.8027\n",
      "Batch 63 loss: 0.7526\n",
      "Batch 64 loss: 0.7166\n",
      "Batch 65 loss: 0.6574\n",
      "Batch 66 loss: 0.5818\n",
      "Batch 67 loss: 0.5871\n",
      "Batch 68 loss: 0.5174\n",
      "Batch 69 loss: 0.5723\n",
      "Batch 70 loss: 0.5898\n",
      "Batch 71 loss: 0.4912\n",
      "Batch 72 loss: 0.4831\n",
      "Batch 73 loss: 0.4600\n",
      "Batch 74 loss: 0.4221\n",
      "Batch 75 loss: 0.4066\n",
      "Batch 76 loss: 0.3849\n",
      "Batch 77 loss: 0.3494\n",
      "Batch 78 loss: 0.3996\n",
      "Batch 79 loss: 0.3482\n",
      "Batch 80 loss: 0.3364\n",
      "Batch 81 loss: 0.3043\n",
      "Batch 82 loss: 0.3392\n",
      "Batch 83 loss: 0.3158\n",
      "Batch 84 loss: 0.3413\n",
      "Batch 85 loss: 0.2808\n",
      "Batch 86 loss: 0.3494\n",
      "Batch 87 loss: 0.2842\n",
      "Batch 88 loss: 0.3183\n",
      "Batch 89 loss: 0.2507\n",
      "Batch 90 loss: 0.2989\n",
      "Batch 91 loss: 0.2462\n",
      "Batch 92 loss: 0.3189\n",
      "Batch 93 loss: 0.2816\n",
      "Batch 94 loss: 0.2549\n",
      "Batch 95 loss: 0.2653\n",
      "Batch 96 loss: 0.2776\n",
      "Batch 97 loss: 0.2400\n",
      "Batch 98 loss: 0.2522\n",
      "Batch 99 loss: 0.2477\n",
      "Batch 100 loss: 0.2382\n",
      "Batch 101 loss: 0.2521\n"
     ]
    }
   ],
   "source": [
    "model, train_results = train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
