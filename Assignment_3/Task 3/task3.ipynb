{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Descriptions (First 5 Records):\n",
      "931874353976938497: people sitting on the floor in a large room with a wall\n",
      "880425829246922752: two twee screens of donald trump and donald trump\n",
      "690915881082343424: there are two shovels that are standing in the snow\n",
      "915228456757059585: arafed view of a passenger plane with a flat screen tv\n",
      "494194068998468686_25639236: cars are driving down the highway on a cloudy day\n",
      "\n",
      "Detected Objects (First 5 Records):\n",
      "931874353976938497: {'classes': ['person', 'backpack', 'handbag', 'backpack', 'backpack', 'cell phone', 'person', 'person', 'person', 'cup', 'chair', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'chair', 'person', 'person', 'person', 'backpack', 'backpack', 'person', 'person', 'person', 'backpack', 'person', 'person', 'person', 'person', 'person', 'person'], 'confidence_scores': [0.0945774, 0.0975185, 0.111666, 0.117207, 0.118647, 0.120145, 0.121325, 0.217946, 0.224531, 0.226831, 0.280922, 0.29603, 0.338583, 0.342417, 0.345206, 0.39319, 0.414227, 0.458103, 0.46252, 0.46994, 0.560848, 0.581421, 0.591951, 0.597088, 0.614235, 0.66626, 0.672012, 0.706699, 0.722024, 0.725346, 0.774559, 0.795291, 0.842849, 0.894795, 0.895111, 0.928385]}\n",
      "880425829246922752: {'classes': ['tv', 'book', 'person', 'person'], 'confidence_scores': [0.0566157, 0.0645416, 0.787489, 0.796628]}\n",
      "915228456757059585: {'classes': ['tv', 'chair', 'tv', 'chair', 'chair', 'person', 'tv', 'chair', 'chair', 'tv', 'tv', 'chair', 'chair', 'tv', 'person', 'chair', 'chair', 'person', 'tv', 'person', 'person', 'tv'], 'confidence_scores': [0.0644448, 0.0680951, 0.0796244, 0.101709, 0.134147, 0.140395, 0.154054, 0.190344, 0.214901, 0.244845, 0.312947, 0.48098, 0.596227, 0.775801, 0.79419, 0.829292, 0.831051, 0.836855, 0.849639, 0.865242, 0.867333, 0.916105]}\n",
      "494194068998468686_25639236: {'classes': ['car', 'car', 'car', 'car', 'car', 'truck', 'truck', 'car', 'truck', 'truck', 'car', 'car', 'truck', 'truck', 'car', 'car'], 'confidence_scores': [0.105599, 0.138541, 0.15192, 0.183974, 0.193817, 0.216392, 0.399908, 0.465357, 0.471086, 0.49458, 0.577627, 0.634711, 0.830119, 0.863202, 0.874461, 0.91342]}\n",
      "886345583052681217: {'classes': ['boat', 'boat', 'boat', 'boat', 'bench', 'boat', 'boat', 'bench', 'bottle', 'boat', 'boat', 'bench', 'boat', 'chair'], 'confidence_scores': [0.0647115, 0.0925675, 0.178609, 0.269199, 0.305418, 0.328585, 0.341485, 0.358296, 0.362327, 0.450375, 0.514758, 0.572053, 0.582355, 0.91542]}\n"
     ]
    }
   ],
   "source": [
    "# text data\n",
    "df = pd.read_csv(\"train_df.tsv\", sep=\"\\t\")\n",
    "\n",
    "# img descriptions\n",
    "with open(\"D_train.pkl\", \"rb\") as f:\n",
    "    image_descriptions = pickle.load(f)\n",
    "print(\"Image Descriptions (First 5 Records):\")\n",
    "for key, value in list(image_descriptions.items())[:5]:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# detected objects\n",
    "with open(\"O_train.pkl\", \"rb\") as f:\n",
    "    detected_objects = pickle.load(f)\n",
    "print(\"\\nDetected Objects (First 5 Records):\")\n",
    "for key, value in list(detected_objects.items())[:5]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "\n",
    "class MuSEDataset(Dataset):\n",
    "    def __init__(self, text_file, image_desc_file, obj_file, image_folder, tokenizer, transform=None):\n",
    "        ''' Initialize the MuSEDataset class. '''\n",
    "        self.text_data = pd.read_csv(text_file, sep=\"\\t\")\n",
    "        \n",
    "        with open(image_desc_file, \"rb\") as f:\n",
    "            self.image_descriptions = pickle.load(f)\n",
    "        \n",
    "        with open(obj_file, \"rb\") as f:\n",
    "            self.detected_objects = pickle.load(f)\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization - mean and std dev for RGB channels\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return the length of the dataset. '''\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Retrieve the data for a given index. '''\n",
    "\n",
    "        # Get data\n",
    "        row = self.text_data.iloc[idx]\n",
    "        text = row[\"text\"]  \n",
    "        image_name = str(row[\"pid\"])  # Convert to string to match dictionary keys\n",
    "        sarcasm_target = str(row[\"target_of_sarcasm\"]) if pd.notna(row[\"target_of_sarcasm\"]) else \"\"\n",
    "\n",
    "        # Load and preprocess image\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image {image_path} not found, using blank image.\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        # Get image description and detected objects (handle missing keys)\n",
    "        img_desc = self.image_descriptions.get(image_name, \"No description available\")\n",
    "        detected_objs = self.detected_objects.get(image_name, \"No objects detected\")\n",
    "\n",
    "        # Tokenize text\n",
    "        text_inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        sarcasm_target_inputs = self.tokenizer(sarcasm_target, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"text_input_ids\": text_inputs[\"input_ids\"].squeeze(0),\n",
    "            \"text_attention_mask\": text_inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"image\": image,\n",
    "            \"image_description\": img_desc,\n",
    "            \"detected_objects\": detected_objs,\n",
    "            \"sarcasm_target_input_ids\": sarcasm_target_inputs[\"input_ids\"].squeeze(0),\n",
    "            \"sarcasm_target_attention_mask\": sarcasm_target_inputs[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "# Load the BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = MuSEDataset(\"train_df.tsv\", \"D_train.pkl\", \"O_train.pkl\", \"images\", tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import ViTModel, ViTFeatureExtractor\n",
    "\n",
    "# # Create ViT class that will get image embeddings\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"facebook/vit-base-patch16-224\")\n",
    "# vit_model = ViTModel.from_pretrained(\"facebook/vit-base-patch16-224\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16\n",
    "import torch\n",
    "\n",
    "# Load pretrained ViT model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit = vit_b_16(weights=\"IMAGENET1K_V1\")  # Load pretrained weights\n",
    "vit.heads = nn.Identity()  # Remove classification head\n",
    "vit = vit.to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process one batch\n",
    "for batch in dataloader:\n",
    "    batch[\"image\"] = batch[\"image\"].to(device)  # Move images to GPU\n",
    "    image_embeddings = vit(batch[\"image\"])  # Extract features\n",
    "    print(image_embeddings.shape)  # Expected output: (batch_size, 768)\n",
    "    break  # Stop after one batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
