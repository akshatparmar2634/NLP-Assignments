{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Descriptions (First 5 Records):\n",
      "931874353976938497: people sitting on the floor in a large room with a wall\n",
      "880425829246922752: two twee screens of donald trump and donald trump\n",
      "690915881082343424: there are two shovels that are standing in the snow\n",
      "915228456757059585: arafed view of a passenger plane with a flat screen tv\n",
      "494194068998468686_25639236: cars are driving down the highway on a cloudy day\n",
      "\n",
      "Detected Objects (First 5 Records):\n",
      "931874353976938497: {'classes': ['person', 'backpack', 'handbag', 'backpack', 'backpack', 'cell phone', 'person', 'person', 'person', 'cup', 'chair', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'chair', 'person', 'person', 'person', 'backpack', 'backpack', 'person', 'person', 'person', 'backpack', 'person', 'person', 'person', 'person', 'person', 'person'], 'confidence_scores': [0.0945774, 0.0975185, 0.111666, 0.117207, 0.118647, 0.120145, 0.121325, 0.217946, 0.224531, 0.226831, 0.280922, 0.29603, 0.338583, 0.342417, 0.345206, 0.39319, 0.414227, 0.458103, 0.46252, 0.46994, 0.560848, 0.581421, 0.591951, 0.597088, 0.614235, 0.66626, 0.672012, 0.706699, 0.722024, 0.725346, 0.774559, 0.795291, 0.842849, 0.894795, 0.895111, 0.928385]}\n",
      "880425829246922752: {'classes': ['tv', 'book', 'person', 'person'], 'confidence_scores': [0.0566157, 0.0645416, 0.787489, 0.796628]}\n",
      "915228456757059585: {'classes': ['tv', 'chair', 'tv', 'chair', 'chair', 'person', 'tv', 'chair', 'chair', 'tv', 'tv', 'chair', 'chair', 'tv', 'person', 'chair', 'chair', 'person', 'tv', 'person', 'person', 'tv'], 'confidence_scores': [0.0644448, 0.0680951, 0.0796244, 0.101709, 0.134147, 0.140395, 0.154054, 0.190344, 0.214901, 0.244845, 0.312947, 0.48098, 0.596227, 0.775801, 0.79419, 0.829292, 0.831051, 0.836855, 0.849639, 0.865242, 0.867333, 0.916105]}\n",
      "494194068998468686_25639236: {'classes': ['car', 'car', 'car', 'car', 'car', 'truck', 'truck', 'car', 'truck', 'truck', 'car', 'car', 'truck', 'truck', 'car', 'car'], 'confidence_scores': [0.105599, 0.138541, 0.15192, 0.183974, 0.193817, 0.216392, 0.399908, 0.465357, 0.471086, 0.49458, 0.577627, 0.634711, 0.830119, 0.863202, 0.874461, 0.91342]}\n",
      "886345583052681217: {'classes': ['boat', 'boat', 'boat', 'boat', 'bench', 'boat', 'boat', 'bench', 'bottle', 'boat', 'boat', 'bench', 'boat', 'chair'], 'confidence_scores': [0.0647115, 0.0925675, 0.178609, 0.269199, 0.305418, 0.328585, 0.341485, 0.358296, 0.362327, 0.450375, 0.514758, 0.572053, 0.582355, 0.91542]}\n"
     ]
    }
   ],
   "source": [
    "# text data\n",
    "df = pd.read_csv(\"train_df.tsv\", sep=\"\\t\") \n",
    "\n",
    "# img descriptions\n",
    "with open(\"D_train.pkl\", \"rb\") as f:\n",
    "    image_descriptions = pickle.load(f)\n",
    "print(\"Image Descriptions (First 5 Records):\")\n",
    "for key, value in list(image_descriptions.items())[:5]:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# detected objects\n",
    "with open(\"O_train.pkl\", \"rb\") as f:\n",
    "    detected_objects = pickle.load(f)\n",
    "print(\"Detected Objects (First 5 Records):\")\n",
    "for key, value in list(detected_objects.items())[:5]:\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, ViTModel\n",
    "from torchvision.models import vit_b_16\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuSEDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text_file, image_desc_file, obj_file, image_folder, tokenizer, max_length=256, transform=None):\n",
    "        ''' Initialize the MuSEDataset class. '''\n",
    "        self.text_data = pd.read_csv(text_file, sep=\"\\t\")\n",
    "        \n",
    "        with open(image_desc_file, \"rb\") as f:\n",
    "            self.image_descriptions = pickle.load(f)\n",
    "        \n",
    "        with open(obj_file, \"rb\") as f:\n",
    "            self.detected_objects = pickle.load(f)\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "            # mean and std dev for RGB channels\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        ''' Return the length of the dataset. '''\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the data for a given index.\n",
    "        Here, we get:\n",
    "        1. Multimodal input (txt, img desc, obj)\n",
    "        2. Explanantion and target\n",
    "        Tokenize them and return a dictionary of relevant data\n",
    "        '''        # Get data\n",
    "        row = self.text_data.iloc[idx]\n",
    "        text = row[\"text\"]  \n",
    "        explanation = row[\"explanation\"] if \"explanation\" in row else \"\"\n",
    "        image_name = str(row[\"pid\"])  # Convert to string to match dictionary keys\n",
    "        sarcasm_target = str(row[\"target_of_sarcasm\"]) if pd.notna(row.get(\"target_of_sarcasm\", \"\")) else \"\"\n",
    "\n",
    "        # Preprocess img\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_name}.jpg\")\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            # blank image if not found\n",
    "            print(f\"Warning: Image {image_path} not found, using blank image.\")\n",
    "            image = torch.zeros((3, 224, 224))\n",
    "\n",
    "        # img desc and objects\n",
    "        img_desc = self.image_descriptions.get(image_name, \"No description available\")\n",
    "        detected_objs = self.detected_objects.get(image_name, {\"classes\": [], \"confidence_scores\": []})\n",
    "        \n",
    "        # formatting objs to string\n",
    "        if isinstance(detected_objs, dict) and \"classes\" in detected_objs:\n",
    "            obj_str = \", \".join(detected_objs[\"classes\"])\n",
    "        else:\n",
    "            obj_str = \"No objects detected\"\n",
    "\n",
    "        # multimodal input\n",
    "        multimodal_text = f\"Text: {text} Image: {img_desc} Objects: {obj_str}\"\n",
    "        \n",
    "        # Tokenize inputs, explanation, target\n",
    "        input_encodings = self.tokenizer(multimodal_text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encodings = self.tokenizer(explanation, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        sarcasm_target_encodings = self.tokenizer(sarcasm_target, padding='max_length', truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # Return the data as a dictionary\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"image\": image,\n",
    "            \"target_ids\": target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"target_attention_mask\": target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"sarcasm_target_ids\": sarcasm_target_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"sarcasm_target_mask\": sarcasm_target_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"raw_text\": text,\n",
    "            \"raw_explanation\": explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProjection(nn.Module):\n",
    "    \"\"\"Projects ViT image features to match BART embedding dimensions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=768, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Computes cross-modal attention between text and vision features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_v = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_v = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.query_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.key_t = nn.Linear(feature_dim, feature_dim)\n",
    "        self.value_t = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.scale = feature_dim ** 0.5\n",
    "        \n",
    "    def forward(self, vision_features, text_features):\n",
    "        # self-attention for vision \n",
    "        q_v = self.query_v(vision_features)\n",
    "        k_v = self.key_v(vision_features)\n",
    "        v_v = self.value_v(vision_features)\n",
    "        \n",
    "        attn_v = torch.matmul(q_v, k_v.transpose(-2, -1)) / self.scale\n",
    "        attn_v = F.softmax(attn_v, dim=-1)\n",
    "        attn_v = self.dropout(attn_v)\n",
    "        Av = torch.matmul(attn_v, v_v)\n",
    "        \n",
    "        # self-attention for text \n",
    "        q_t = self.query_t(text_features)\n",
    "        k_t = self.key_t(text_features)\n",
    "        v_t = self.value_t(text_features)\n",
    "        \n",
    "        attn_t = torch.matmul(q_t, k_t.transpose(-2, -1)) / self.scale\n",
    "        attn_t = F.softmax(attn_t, dim=-1)\n",
    "        attn_t = self.dropout(attn_t)\n",
    "        At = torch.matmul(attn_t, v_t)\n",
    "        \n",
    "        # cross modal\n",
    "        Ftv = At * vision_features\n",
    "        Fvt = Av * text_features\n",
    "        \n",
    "        return Ftv, Fvt\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Gated fusion mechanism to dynamically weigh features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(feature_dim, feature_dim)\n",
    "        self.Wt = nn.Linear(feature_dim, feature_dim)\n",
    "        self.bv = nn.Parameter(torch.zeros(feature_dim))\n",
    "        self.bt = nn.Parameter(torch.zeros(feature_dim))\n",
    "    \n",
    "    def forward(self, Ev, Et, Ftv, Fvt):\n",
    "        # Gate for controlling information flow\n",
    "        Gv = torch.sigmoid(self.Wv(Ev) + self.bv)\n",
    "        Gt = torch.sigmoid(self.Wt(Et) + self.bt)\n",
    "        \n",
    "        # Multimodal features\n",
    "        F1 = (Gv * Ftv) + ((1 - Gv) * Fvt)\n",
    "        F2 = (Gt * Ftv) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        # Unimodal features\n",
    "        Fv = (Gv * Ev) + ((1 - Gv) * Ftv)\n",
    "        Ft = (Gt * Et) + ((1 - Gt) * Fvt)\n",
    "\n",
    "        return F1, F2, Fv, Ft\n",
    "\n",
    "class SharedFusion(nn.Module):\n",
    "    \"\"\"Shared fusion mechanism that combines multimodal and unimodal features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.alpha1 = nn.Parameter(torch.ones(1))\n",
    "        self.alpha2 = nn.Parameter(torch.ones(1))\n",
    "        self.beta1 = nn.Parameter(torch.ones(1))\n",
    "        self.beta2 = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, F1, F2, Fv, Ft):\n",
    "        # Linear combination\n",
    "        FSF = self.alpha1 * F1 + self.alpha2 * F2 + self.beta1 * Fv + self.beta2 * Ft\n",
    "        return FSF\n",
    "\n",
    "class MuSEModel(nn.Module):\n",
    "    \"\"\"Complete MuSE model for sarcasm explanation generation\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=768, max_length=256):\n",
    "        super().__init__()\n",
    "        # Vision - ViT\n",
    "        self.vit = vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "        self.vit.heads = nn.Identity()  # Remove classification head\n",
    "        self.image_projection = ImageProjection(feature_dim, feature_dim)\n",
    "        \n",
    "        # Text - BART\n",
    "        self.bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_modal_attention = CrossModalAttention(feature_dim)\n",
    "        \n",
    "        # Fusion components\n",
    "        self.gated_fusion = GatedFusion(feature_dim)\n",
    "        self.shared_fusion = SharedFusion(feature_dim)\n",
    "        \n",
    "        # Dimensions and positioning\n",
    "        self.feature_dim = feature_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Target information projection \n",
    "        self.target_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        # Final projection (match BART dimensions)\n",
    "        self.output_projection = nn.Linear(feature_dim, feature_dim)\n",
    "        \n",
    "        print(\"Initializing MuSEModel...\")\n",
    "        \n",
    "    def embed_text(self, input_ids, attention_mask):\n",
    "        \"\"\"Extract text embeddings using BART encoder\"\"\"\n",
    "        encoder_outputs = self.bart.get_encoder()(  # Direct encoder access\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return encoder_outputs.last_hidden_state\n",
    "    \n",
    "    def embed_image(self, images):\n",
    "        \"\"\"Extract image embeddings using ViT\"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = self.vit(images)\n",
    "        return self.image_projection(image_features)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, images, target_ids=None, sarcasm_target_ids=None, sarcasm_target_mask=None):\n",
    "        '''\n",
    "        Et - text embeddings\n",
    "        Ev - vision embeddings\n",
    "        \n",
    "        Ftv, Fvt - cross modal attention(Et,Ev)\n",
    "        \n",
    "        F1, F2, Fv, Ft - gated fusion(Ev, Et, Ftv, Fvt)\n",
    "        FSF - shared fusion(F1, F2, Fv, Ft)\n",
    "        Z - final output\n",
    "        '''\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "        \n",
    "        # Et is embeddings for (text + target)\n",
    "        \n",
    "        Et = self.embed_text(input_ids, attention_mask)\n",
    "        if sarcasm_target_ids is not None and sarcasm_target_mask is not None:\n",
    "            target_embed = self.embed_text(sarcasm_target_ids, sarcasm_target_mask)\n",
    "            target_embed = self.target_projection(target_embed)\n",
    "            Et = Et + target_embed\n",
    "        \n",
    "        # Ev - vision embeddings -> reshape to match text embeddings size   \n",
    "        Ev = self.embed_image(images)\n",
    "        Ev = Ev.unsqueeze(1).expand(-1, self.max_length, -1)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        Ftv, Fvt = self.cross_modal_attention(Ev, Et)\n",
    "        \n",
    "        # Gated fusion\n",
    "        F1, F2, Fv, Ft = self.gated_fusion(Ev, Et, Ftv, Fvt)\n",
    "        \n",
    "        # Shared fusion\n",
    "        FSF = self.shared_fusion(F1, F2, Fv, Ft)\n",
    "        \n",
    "        # Final proj\n",
    "        Z = self.output_projection(FSF) + Et \n",
    "    \n",
    "        if target_ids is not None:\n",
    "            \n",
    "            target_length = target_ids.size(1)\n",
    "            Z = Z[:, :target_length, :]\n",
    "            \n",
    "            # Pass to BART with proper dimensions\n",
    "            outputs = self.bart(\n",
    "                encoder_outputs=(Z,),\n",
    "                attention_mask=attention_mask,\n",
    "                labels=target_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            return outputs\n",
    "        else:\n",
    "            # Inference mode - generate text\n",
    "            generated_ids = self.bart.generate(\n",
    "                encoder_outputs=(Z,),  # Proper tuple format\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    print(f\"Total number of batches: {num_batches}\")\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        raise ValueError(\"Dataloader is empty!\")\n",
    "        \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        try:\n",
    "            \n",
    "            # Move data to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            print(f\"Batch {batch_idx} loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            raise e\n",
    "            \n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch complete. Average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, tokenizer, device, num_samples=5):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    generated_explanations = []\n",
    "    reference_explanations = []\n",
    "    sample_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Move inputs to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            images = batch[\"image\"].to(device)\n",
    "            target_ids = batch[\"target_ids\"].to(device)\n",
    "            sarcasm_target_ids = batch[\"sarcasm_target_ids\"].to(device)\n",
    "            sarcasm_target_mask = batch[\"sarcasm_target_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass with teacher forcing for loss calculation\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                images=images,\n",
    "                target_ids=target_ids,\n",
    "                sarcasm_target_ids=sarcasm_target_ids,\n",
    "                sarcasm_target_mask=sarcasm_target_mask\n",
    "            )\n",
    "            \n",
    "            # Get loss\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            #  get explanations\n",
    "            generated_ids = model.bart.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=100,\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # decode explanations\n",
    "            for i in range(input_ids.size(0)):\n",
    "                gen_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(target_ids[i], skip_special_tokens=True)\n",
    "                \n",
    "                generated_explanations.append(gen_text)\n",
    "                reference_explanations.append(ref_text)\n",
    "                \n",
    "                # Save sample results for display\n",
    "                if len(sample_results) < num_samples:\n",
    "                    original_text = batch[\"raw_text\"][i]\n",
    "                    sample_results.append({\n",
    "                        \"text\": original_text,\n",
    "                        \"generated\": gen_text,\n",
    "                        \"reference\": ref_text\n",
    "                    })\n",
    "    \n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=reference_explanations\n",
    "    )\n",
    "    \n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    bleu_results = bleu.compute(\n",
    "        predictions=generated_explanations,\n",
    "        references=[[r] for r in reference_explanations]\n",
    "    )\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = val_loss / len(dataloader)\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"rouge\": rouge_results,\n",
    "        \"bleu\": bleu_results,\n",
    "        \"samples\": sample_results[:5]\n",
    "    }\n",
    "\n",
    "def train_model(model, train_loader, val_loader, tokenizer, device, \n",
    "                 epochs=20, lr=0.001, checkpoint_dir=\"checkpoints\"):\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    rouge_scores = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # train - 1 epoch\n",
    "        print(f\"Training on epoch {epoch + 1}...\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Training Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # validate\n",
    "        val_results = validate(model, val_loader, tokenizer, device)\n",
    "        val_losses.append(val_results[\"loss\"])\n",
    "        rouge_scores.append(val_results[\"rouge\"][\"rougeL\"])\n",
    "        bleu_scores.append(val_results[\"bleu\"][\"bleu\"])\n",
    "        \n",
    "        print(f\"Validation Loss: {val_results['loss']:.4f}\")\n",
    "        print(f\"ROUGE-L: {val_results['rouge']['rougeL']:.4f}\")\n",
    "        print(f\"BLEU: {val_results['bleu']['bleu']:.4f}\")\n",
    "        \n",
    "        # sample results\n",
    "        print(\"\\nSample Results:\")\n",
    "        for i, sample in enumerate(val_results[\"samples\"]):\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"Text: {sample['text']}\")\n",
    "            print(f\"Generated: {sample['generated']}\")\n",
    "            print(f\"Reference: {sample['reference']}\")\n",
    "            print()\n",
    "        \n",
    "        # save checkpt\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"muse_model_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_results[\"loss\"],\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(rouge_scores)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ROUGE-L')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(bleu_scores)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('BLEU')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, 'training_progress.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'rouge_scores': rouge_scores,\n",
    "        'bleu_scores': bleu_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def train_and_validate():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # BART tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    from torch.utils.data import Subset\n",
    "\n",
    "    # Full training dataset\n",
    "    train_dataset = MuSEDataset(\n",
    "        text_file=\"train_df.tsv\", \n",
    "        image_desc_file=\"D_train.pkl\", \n",
    "        obj_file=\"O_train.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # Create a subset of the first 100 samples for training\n",
    "    train_indices = list(range(100))\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # dataloaders\n",
    "    batch_size = 16\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_loader.dataset)}\")\n",
    "    print(f\"Number of batches: {len(train_loader)}\")\n",
    "        \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(len(val_loader.dataset))\n",
    "\n",
    "        # model\n",
    "    model = MuSEModel().to(device)\n",
    "    \n",
    "    # train\n",
    "    train_results = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        epochs=20,\n",
    "        lr=1e-4,\n",
    "        checkpoint_dir=\"checkpoints\"\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    print(\"Saving final model...\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = os.path.join(\"checkpoints\", \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model, train_results\n",
    "\n",
    "def test_model(model_path=\"checkpoints/final_model.pt\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "    \n",
    "    # Load model\n",
    "    model = MuSEModel().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test data\n",
    "    test_dataset = MuSEDataset(\n",
    "        text_file=\"test_df.tsv\", \n",
    "        image_desc_file=\"D_test.pkl\", \n",
    "        obj_file=\"O_test.pkl\", \n",
    "        image_folder=\"images\", \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    test_results = validate(model, test_loader, tokenizer, device)\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to(device),\n",
    "                attention_mask=batch['attention_mask'].to(device),\n",
    "                max_length=50\n",
    "            )\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            predictions.extend(preds)\n",
    "    \n",
    "    with open(\"test_predictions.txt\", \"w\") as f:\n",
    "        for pred in predictions:\n",
    "            f.write(pred + \"\\n\")\n",
    "    \n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Loss: {test_results['loss']:.4f}\")\n",
    "    print(f\"ROUGE-1: {test_results['rouge']['rouge1']:.4f}\")\n",
    "    print(f\"ROUGE-2: {test_results['rouge']['rouge2']:.4f}\")\n",
    "    print(f\"ROUGE-L: {test_results['rouge']['rougeL']:.4f}\")\n",
    "    print(f\"BLEU: {test_results['bleu']['bleu']:.4f}\")\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train dataset size: 2684\n",
      "Number of batches: 168\n",
      "299\n",
      "Initializing MuSEModel...\n",
      "Epoch 1/20\n",
      "Training on epoch 1...\n",
      "Total number of batches: 168\n",
      "Batch 0 loss: 16.5058\n",
      "Batch 1 loss: 13.4759\n",
      "Batch 2 loss: 12.6706\n",
      "Batch 3 loss: 12.1128\n",
      "Batch 4 loss: 11.7490\n",
      "Batch 5 loss: 11.3674\n",
      "Batch 6 loss: 10.9471\n",
      "Batch 7 loss: 10.5997\n",
      "Batch 8 loss: 10.1041\n",
      "Batch 9 loss: 9.6193\n",
      "Batch 10 loss: 9.1360\n",
      "Batch 11 loss: 8.4978\n",
      "Batch 12 loss: 7.7686\n",
      "Batch 13 loss: 7.2001\n",
      "Batch 14 loss: 6.4611\n",
      "Batch 15 loss: 6.0651\n",
      "Batch 16 loss: 5.5280\n",
      "Batch 17 loss: 5.1871\n",
      "Batch 18 loss: 4.7313\n",
      "Batch 19 loss: 4.4671\n",
      "Batch 20 loss: 4.1908\n",
      "Batch 21 loss: 3.7704\n",
      "Batch 22 loss: 3.5746\n",
      "Batch 23 loss: 3.2782\n",
      "Batch 24 loss: 2.9736\n",
      "Batch 25 loss: 2.8126\n",
      "Batch 26 loss: 2.5531\n",
      "Batch 27 loss: 2.3761\n",
      "Batch 28 loss: 2.1015\n",
      "Batch 29 loss: 1.8180\n",
      "Batch 30 loss: 1.7454\n",
      "Batch 31 loss: 1.4362\n",
      "Batch 32 loss: 1.3554\n",
      "Batch 33 loss: 1.1585\n",
      "Batch 34 loss: 1.0549\n",
      "Batch 35 loss: 1.0237\n",
      "Batch 36 loss: 0.8793\n",
      "Batch 37 loss: 0.8113\n",
      "Batch 38 loss: 0.7587\n",
      "Batch 39 loss: 0.6278\n",
      "Batch 40 loss: 0.6076\n",
      "Batch 41 loss: 0.6753\n",
      "Batch 42 loss: 0.5435\n",
      "Batch 43 loss: 0.6002\n",
      "Batch 44 loss: 0.5568\n",
      "Batch 45 loss: 0.4670\n",
      "Batch 46 loss: 0.4190\n",
      "Batch 47 loss: 0.4829\n",
      "Batch 48 loss: 0.4152\n",
      "Batch 49 loss: 0.5108\n",
      "Batch 50 loss: 0.4820\n",
      "Batch 51 loss: 0.4191\n",
      "Batch 52 loss: 0.3834\n",
      "Batch 53 loss: 0.4484\n",
      "Batch 54 loss: 0.3565\n",
      "Batch 55 loss: 0.4309\n",
      "Batch 56 loss: 0.3476\n",
      "Batch 57 loss: 0.4150\n",
      "Batch 58 loss: 0.3738\n",
      "Batch 59 loss: 0.3469\n",
      "Batch 60 loss: 0.3694\n",
      "Batch 61 loss: 0.4242\n",
      "Batch 62 loss: 0.3453\n",
      "Batch 63 loss: 0.3563\n",
      "Batch 64 loss: 0.4207\n",
      "Batch 65 loss: 0.2906\n",
      "Batch 66 loss: 0.2654\n",
      "Batch 67 loss: 0.2979\n",
      "Batch 68 loss: 0.2365\n",
      "Batch 69 loss: 0.3126\n",
      "Batch 70 loss: 0.3483\n",
      "Batch 71 loss: 0.2819\n",
      "Batch 72 loss: 0.2763\n",
      "Batch 73 loss: 0.2831\n",
      "Batch 74 loss: 0.2620\n",
      "Batch 75 loss: 0.2548\n",
      "Batch 76 loss: 0.2348\n",
      "Batch 77 loss: 0.2062\n",
      "Batch 78 loss: 0.2617\n",
      "Batch 79 loss: 0.2072\n",
      "Batch 80 loss: 0.2439\n",
      "Batch 81 loss: 0.1730\n",
      "Batch 82 loss: 0.2440\n",
      "Batch 83 loss: 0.2235\n",
      "Batch 84 loss: 0.2261\n",
      "Batch 85 loss: 0.2131\n",
      "Batch 86 loss: 0.2466\n",
      "Batch 87 loss: 0.1931\n",
      "Batch 88 loss: 0.2296\n",
      "Batch 89 loss: 0.1584\n",
      "Batch 90 loss: 0.2147\n",
      "Batch 91 loss: 0.1689\n",
      "Batch 92 loss: 0.2381\n",
      "Batch 93 loss: 0.2044\n",
      "Batch 94 loss: 0.1735\n",
      "Batch 95 loss: 0.1843\n",
      "Batch 96 loss: 0.2188\n",
      "Batch 97 loss: 0.1799\n",
      "Batch 98 loss: 0.1802\n",
      "Batch 99 loss: 0.1756\n",
      "Batch 100 loss: 0.1868\n",
      "Batch 101 loss: 0.1880\n",
      "Batch 102 loss: 0.1753\n",
      "Batch 103 loss: 0.2113\n",
      "Batch 104 loss: 0.2038\n",
      "Batch 105 loss: 0.2149\n",
      "Batch 106 loss: 0.1443\n",
      "Batch 107 loss: 0.1628\n",
      "Batch 108 loss: 0.1896\n",
      "Batch 109 loss: 0.1778\n",
      "Batch 110 loss: 0.2301\n",
      "Batch 111 loss: 0.1787\n",
      "Batch 112 loss: 0.1769\n",
      "Batch 113 loss: 0.1777\n",
      "Batch 114 loss: 0.1849\n",
      "Batch 115 loss: 0.1819\n",
      "Batch 116 loss: 0.1729\n",
      "Batch 117 loss: 0.2109\n",
      "Batch 118 loss: 0.1677\n",
      "Batch 119 loss: 0.1517\n",
      "Batch 120 loss: 0.1852\n",
      "Batch 121 loss: 0.1783\n",
      "Batch 122 loss: 0.1476\n",
      "Batch 123 loss: 0.1979\n",
      "Batch 124 loss: 0.1645\n",
      "Batch 125 loss: 0.1700\n",
      "Batch 126 loss: 0.1474\n",
      "Batch 127 loss: 0.1600\n",
      "Batch 128 loss: 0.1317\n",
      "Batch 129 loss: 0.1644\n",
      "Batch 130 loss: 0.1735\n",
      "Batch 131 loss: 0.1711\n",
      "Batch 132 loss: 0.1773\n",
      "Batch 133 loss: 0.1592\n",
      "Batch 134 loss: 0.1827\n",
      "Batch 135 loss: 0.1505\n",
      "Batch 136 loss: 0.1543\n",
      "Batch 137 loss: 0.1424\n",
      "Batch 138 loss: 0.1718\n",
      "Batch 139 loss: 0.1522\n",
      "Batch 140 loss: 0.1529\n",
      "Batch 141 loss: 0.1453\n",
      "Batch 142 loss: 0.1725\n",
      "Batch 143 loss: 0.1373\n",
      "Batch 144 loss: 0.1473\n",
      "Batch 145 loss: 0.1414\n",
      "Batch 146 loss: 0.1729\n",
      "Batch 147 loss: 0.1193\n",
      "Batch 148 loss: 0.1985\n",
      "Batch 149 loss: 0.1527\n",
      "Batch 150 loss: 0.1433\n",
      "Batch 151 loss: 0.1653\n",
      "Batch 152 loss: 0.1297\n",
      "Batch 153 loss: 0.1647\n",
      "Batch 154 loss: 0.1716\n",
      "Batch 155 loss: 0.1165\n",
      "Batch 156 loss: 0.1360\n",
      "Batch 157 loss: 0.1433\n",
      "Batch 158 loss: 0.1988\n",
      "Batch 159 loss: 0.1581\n",
      "Batch 160 loss: 0.1568\n",
      "Batch 161 loss: 0.1303\n",
      "Batch 162 loss: 0.1177\n",
      "Batch 163 loss: 0.1934\n",
      "Batch 164 loss: 0.1581\n",
      "Batch 165 loss: 0.1792\n",
      "Batch 166 loss: 0.1498\n",
      "Batch 167 loss: 0.1092\n",
      "Epoch complete. Average loss: 1.5181\n",
      "Training Loss: 1.5181\n",
      "Validation Loss: 0.1343\n",
      "ROUGE-L: 0.3936\n",
      "BLEU: 0.2554\n",
      "\n",
      "Sample Results:\n",
      "Example 1:\n",
      "Text: 'menu testing with all of the duck ! this is the hard part . # toughwork  # menu # brewpub # openingsoon'\n",
      "Generated: the author is pissed at <user> for themenu testing with all of the duck.\n",
      "Reference: menu testing with all of the duck isn't the hard part, it's fun.\n",
      "\n",
      "Example 2:\n",
      "Text: 'oh great , sky lanterns are sold for all seasons and holidays now ! i just love picking up these ! <user>  # skytrash'\n",
      "Generated: the author is sad that sky lanterns are sold for all seasons and holidays now.\n",
      "Reference: the author hates that sky lanterns are sold for all seasons and holidays now, it's inconvenient picking them up.\n",
      "\n",
      "Example 3:\n",
      "Text: 'so weather conditions are affecting <user> services . it does look dreadful outside ! ! ! # redhillmoo '\n",
      "Generated: the author doesn't think the weather conditions are affecting <user> services.\n",
      "Reference: <user> says weather conditions are affecting their services, but outside looks pretty pleasant to author.\n",
      "\n",
      "Example 4:\n",
      "Text: '<user> <user> nothing says fun like getting ready for a raid at 5:30 in the morning . '\n",
      "Generated: the author doesn't think getting ready for a raid at 5:30 in the morning is fun.\n",
      "Reference: it's not fun getting ready for a raid at 5:30 in the morning.\n",
      "\n",
      "Example 5:\n",
      "Text: 'yeah $ 226 is great ! now i can buy the new # lebrons with all my # profit ... because '\n",
      "Generated: the author doesn't think $ 226 is great since he can buy the new lebrons with all his profit.\n",
      "Reference: the author isn't satisfied with $ 226 since he can't buy the new lebrons with all his profit.\n",
      "\n",
      "Checkpoint saved to checkpoints\\muse_model_epoch_1.pt\n",
      "Epoch 2/20\n",
      "Training on epoch 2...\n",
      "Total number of batches: 168\n",
      "Batch 0 loss: 0.1310\n",
      "Batch 1 loss: 0.0802\n",
      "Batch 2 loss: 0.1437\n",
      "Batch 3 loss: 0.1427\n",
      "Batch 4 loss: 0.1185\n",
      "Batch 5 loss: 0.1353\n",
      "Batch 6 loss: 0.1745\n",
      "Batch 7 loss: 0.1158\n",
      "Batch 8 loss: 0.1498\n",
      "Batch 9 loss: 0.1207\n",
      "Batch 10 loss: 0.1739\n",
      "Batch 11 loss: 0.1074\n",
      "Batch 12 loss: 0.1208\n",
      "Batch 13 loss: 0.1043\n",
      "Batch 14 loss: 0.1221\n",
      "Batch 15 loss: 0.1100\n",
      "Batch 16 loss: 0.1236\n",
      "Batch 17 loss: 0.1842\n",
      "Batch 18 loss: 0.1708\n",
      "Batch 19 loss: 0.1410\n",
      "Batch 20 loss: 0.1162\n",
      "Batch 21 loss: 0.1315\n",
      "Batch 22 loss: 0.1391\n",
      "Batch 23 loss: 0.1094\n",
      "Batch 24 loss: 0.1240\n",
      "Batch 25 loss: 0.1135\n",
      "Batch 26 loss: 0.1084\n",
      "Batch 27 loss: 0.1314\n",
      "Batch 28 loss: 0.1654\n",
      "Batch 29 loss: 0.1078\n",
      "Batch 30 loss: 0.1323\n",
      "Batch 31 loss: 0.1425\n",
      "Batch 32 loss: 0.1539\n",
      "Batch 33 loss: 0.1469\n",
      "Batch 34 loss: 0.1309\n",
      "Batch 35 loss: 0.1354\n",
      "Batch 36 loss: 0.1198\n",
      "Batch 37 loss: 0.1465\n",
      "Batch 38 loss: 0.1438\n",
      "Batch 39 loss: 0.1111\n",
      "Batch 40 loss: 0.1666\n",
      "Batch 41 loss: 0.1551\n",
      "Batch 42 loss: 0.1529\n",
      "Batch 43 loss: 0.1261\n",
      "Batch 44 loss: 0.1323\n",
      "Batch 45 loss: 0.1220\n",
      "Batch 46 loss: 0.1226\n",
      "Batch 47 loss: 0.1329\n",
      "Batch 48 loss: 0.1271\n",
      "Batch 49 loss: 0.1463\n",
      "Batch 50 loss: 0.0874\n",
      "Batch 51 loss: 0.1248\n",
      "Batch 52 loss: 0.1139\n",
      "Batch 53 loss: 0.1298\n",
      "Batch 54 loss: 0.1503\n",
      "Batch 55 loss: 0.1474\n",
      "Batch 56 loss: 0.1270\n",
      "Batch 57 loss: 0.1000\n",
      "Batch 58 loss: 0.1453\n",
      "Batch 59 loss: 0.1573\n",
      "Batch 60 loss: 0.1136\n",
      "Batch 61 loss: 0.1003\n",
      "Batch 62 loss: 0.1132\n",
      "Batch 63 loss: 0.1334\n",
      "Batch 64 loss: 0.1152\n",
      "Batch 65 loss: 0.1189\n",
      "Batch 66 loss: 0.1150\n",
      "Batch 67 loss: 0.1096\n",
      "Batch 68 loss: 0.1423\n",
      "Batch 69 loss: 0.1237\n",
      "Batch 70 loss: 0.1152\n",
      "Batch 71 loss: 0.1136\n",
      "Batch 72 loss: 0.1346\n",
      "Batch 73 loss: 0.0998\n",
      "Batch 74 loss: 0.1333\n",
      "Batch 75 loss: 0.1209\n",
      "Batch 76 loss: 0.1176\n",
      "Batch 77 loss: 0.2149\n",
      "Batch 78 loss: 0.1144\n",
      "Batch 79 loss: 0.1035\n",
      "Batch 80 loss: 0.1206\n",
      "Batch 81 loss: 0.1269\n",
      "Batch 82 loss: 0.1390\n",
      "Batch 83 loss: 0.1354\n",
      "Batch 84 loss: 0.1216\n",
      "Batch 85 loss: 0.1023\n",
      "Batch 86 loss: 0.0966\n",
      "Batch 87 loss: 0.1281\n",
      "Batch 88 loss: 0.1824\n",
      "Batch 89 loss: 0.1109\n",
      "Batch 90 loss: 0.1107\n",
      "Batch 91 loss: 0.1305\n",
      "Batch 92 loss: 0.1488\n",
      "Batch 93 loss: 0.1138\n",
      "Batch 94 loss: 0.1453\n",
      "Batch 95 loss: 0.1228\n",
      "Batch 96 loss: 0.1232\n",
      "Batch 97 loss: 0.1660\n",
      "Batch 98 loss: 0.1775\n",
      "Batch 99 loss: 0.1316\n",
      "Batch 100 loss: 0.1570\n",
      "Batch 101 loss: 0.0925\n",
      "Batch 102 loss: 0.1287\n",
      "Batch 103 loss: 0.1237\n",
      "Batch 104 loss: 0.1164\n",
      "Batch 105 loss: 0.1429\n",
      "Batch 106 loss: 0.1300\n",
      "Batch 107 loss: 0.1501\n",
      "Batch 108 loss: 0.1024\n",
      "Batch 109 loss: 0.1031\n",
      "Batch 110 loss: 0.1172\n",
      "Batch 111 loss: 0.0928\n",
      "Batch 112 loss: 0.0924\n",
      "Batch 113 loss: 0.1151\n",
      "Batch 114 loss: 0.1514\n",
      "Batch 115 loss: 0.1463\n",
      "Batch 116 loss: 0.1261\n",
      "Batch 117 loss: 0.1166\n",
      "Batch 118 loss: 0.1173\n",
      "Batch 119 loss: 0.1072\n",
      "Batch 120 loss: 0.1959\n",
      "Batch 121 loss: 0.1300\n",
      "Batch 122 loss: 0.1364\n",
      "Batch 123 loss: 0.1034\n",
      "Batch 124 loss: 0.1269\n",
      "Batch 125 loss: 0.1379\n",
      "Batch 126 loss: 0.1244\n",
      "Batch 127 loss: 0.1114\n",
      "Batch 128 loss: 0.1308\n",
      "Batch 129 loss: 0.1144\n",
      "Batch 130 loss: 0.1042\n",
      "Batch 131 loss: 0.1309\n",
      "Batch 132 loss: 0.1517\n",
      "Batch 133 loss: 0.1252\n",
      "Batch 134 loss: 0.1025\n",
      "Batch 135 loss: 0.1531\n",
      "Batch 136 loss: 0.1093\n",
      "Batch 137 loss: 0.1303\n",
      "Batch 138 loss: 0.1263\n",
      "Batch 139 loss: 0.1408\n",
      "Batch 140 loss: 0.1273\n",
      "Batch 141 loss: 0.1231\n",
      "Batch 142 loss: 0.1217\n",
      "Batch 143 loss: 0.1212\n",
      "Batch 144 loss: 0.1553\n",
      "Batch 145 loss: 0.1428\n",
      "Batch 146 loss: 0.1373\n",
      "Batch 147 loss: 0.1102\n",
      "Batch 148 loss: 0.1310\n",
      "Batch 149 loss: 0.1165\n",
      "Batch 150 loss: 0.1336\n",
      "Batch 151 loss: 0.1251\n",
      "Batch 152 loss: 0.0960\n",
      "Batch 153 loss: 0.1513\n",
      "Batch 154 loss: 0.1631\n",
      "Batch 155 loss: 0.1093\n",
      "Batch 156 loss: 0.1006\n",
      "Batch 157 loss: 0.1127\n",
      "Batch 158 loss: 0.0960\n",
      "Batch 159 loss: 0.1455\n",
      "Batch 160 loss: 0.1024\n",
      "Batch 161 loss: 0.1287\n",
      "Batch 162 loss: 0.1305\n",
      "Batch 163 loss: 0.1167\n",
      "Batch 164 loss: 0.1603\n",
      "Batch 165 loss: 0.1242\n",
      "Batch 166 loss: 0.1012\n",
      "Batch 167 loss: 0.1349\n",
      "Epoch complete. Average loss: 0.1284\n",
      "Training Loss: 0.1284\n",
      "Validation Loss: 0.1262\n",
      "ROUGE-L: 0.4159\n",
      "BLEU: 0.2675\n",
      "\n",
      "Sample Results:\n",
      "Example 1:\n",
      "Text: 'menu testing with all of the duck ! this is the hard part . # toughwork  # menu # brewpub # openingsoon'\n",
      "Generated: menu testing with all of the duck is the hard part.\n",
      "Reference: menu testing with all of the duck isn't the hard part, it's fun.\n",
      "\n",
      "Example 2:\n",
      "Text: 'oh great , sky lanterns are sold for all seasons and holidays now ! i just love picking up these ! <user>  # skytrash'\n",
      "Generated: the author is sad that sky lanterns are sold for all seasons and holidays now.\n",
      "Reference: the author hates that sky lanterns are sold for all seasons and holidays now, it's inconvenient picking them up.\n",
      "\n",
      "Example 3:\n",
      "Text: 'so weather conditions are affecting <user> services . it does look dreadful outside ! ! ! # redhillmoo '\n",
      "Generated: the author doesn't think the weather conditions are affecting <user> services.\n",
      "Reference: <user> says weather conditions are affecting their services, but outside looks pretty pleasant to author.\n",
      "\n",
      "Example 4:\n",
      "Text: '<user> <user> nothing says fun like getting ready for a raid at 5:30 in the morning . '\n",
      "Generated: the author doesn't like getting ready for a raid at 5:30 in the morning.\n",
      "Reference: it's not fun getting ready for a raid at 5:30 in the morning.\n",
      "\n",
      "Example 5:\n",
      "Text: 'yeah $ 226 is great ! now i can buy the new # lebrons with all my # profit ... because '\n",
      "Generated: the author doesn't think $ 226 is great since he can buy the new lebrons with all his profit.\n",
      "Reference: the author isn't satisfied with $ 226 since he can't buy the new lebrons with all his profit.\n",
      "\n",
      "Checkpoint saved to checkpoints\\muse_model_epoch_2.pt\n",
      "Epoch 3/20\n",
      "Training on epoch 3...\n",
      "Total number of batches: 168\n",
      "Batch 0 loss: 0.0957\n",
      "Batch 1 loss: 0.1271\n",
      "Batch 2 loss: 0.1051\n",
      "Batch 3 loss: 0.0956\n",
      "Batch 4 loss: 0.0965\n",
      "Batch 5 loss: 0.0856\n",
      "Batch 6 loss: 0.1348\n",
      "Batch 7 loss: 0.1272\n",
      "Batch 8 loss: 0.0918\n",
      "Batch 9 loss: 0.0914\n",
      "Batch 10 loss: 0.0927\n",
      "Batch 11 loss: 0.1062\n",
      "Batch 12 loss: 0.1349\n",
      "Batch 13 loss: 0.1264\n",
      "Batch 14 loss: 0.1234\n",
      "Batch 15 loss: 0.1334\n",
      "Batch 16 loss: 0.0931\n",
      "Batch 17 loss: 0.0781\n",
      "Batch 18 loss: 0.0978\n",
      "Batch 19 loss: 0.1162\n",
      "Batch 20 loss: 0.1110\n",
      "Batch 21 loss: 0.1087\n",
      "Batch 22 loss: 0.0948\n",
      "Batch 23 loss: 0.1115\n",
      "Batch 24 loss: 0.0880\n",
      "Batch 25 loss: 0.1292\n",
      "Batch 26 loss: 0.0987\n",
      "Batch 27 loss: 0.0793\n",
      "Batch 28 loss: 0.0881\n",
      "Batch 29 loss: 0.0845\n",
      "Batch 30 loss: 0.1082\n",
      "Batch 31 loss: 0.0959\n",
      "Batch 32 loss: 0.1064\n",
      "Batch 33 loss: 0.1282\n",
      "Batch 34 loss: 0.0952\n",
      "Batch 35 loss: 0.0715\n",
      "Batch 36 loss: 0.0891\n",
      "Batch 37 loss: 0.1375\n",
      "Batch 38 loss: 0.1325\n",
      "Batch 39 loss: 0.1169\n",
      "Batch 40 loss: 0.0861\n",
      "Batch 41 loss: 0.1179\n",
      "Batch 42 loss: 0.1132\n",
      "Batch 43 loss: 0.0965\n",
      "Batch 44 loss: 0.0907\n",
      "Batch 45 loss: 0.0848\n",
      "Batch 46 loss: 0.0908\n",
      "Batch 47 loss: 0.1318\n",
      "Batch 48 loss: 0.1073\n",
      "Batch 49 loss: 0.1195\n",
      "Batch 50 loss: 0.0627\n",
      "Batch 51 loss: 0.1008\n",
      "Batch 52 loss: 0.0795\n",
      "Batch 53 loss: 0.1298\n",
      "Batch 54 loss: 0.0750\n",
      "Batch 55 loss: 0.0840\n",
      "Batch 56 loss: 0.0929\n",
      "Batch 57 loss: 0.1139\n",
      "Batch 58 loss: 0.1084\n",
      "Batch 59 loss: 0.0890\n",
      "Batch 60 loss: 0.1099\n",
      "Batch 61 loss: 0.1449\n",
      "Batch 62 loss: 0.1334\n",
      "Batch 63 loss: 0.0829\n",
      "Batch 64 loss: 0.1013\n",
      "Batch 65 loss: 0.0763\n",
      "Batch 66 loss: 0.1049\n",
      "Batch 67 loss: 0.1363\n",
      "Batch 68 loss: 0.1016\n",
      "Batch 69 loss: 0.0930\n",
      "Batch 70 loss: 0.1253\n",
      "Batch 71 loss: 0.1134\n",
      "Batch 72 loss: 0.0776\n",
      "Batch 73 loss: 0.1035\n",
      "Batch 74 loss: 0.1015\n",
      "Batch 75 loss: 0.0869\n",
      "Batch 76 loss: 0.1233\n",
      "Batch 77 loss: 0.1194\n",
      "Batch 78 loss: 0.0913\n",
      "Batch 79 loss: 0.0807\n",
      "Batch 80 loss: 0.0800\n",
      "Batch 81 loss: 0.0947\n",
      "Batch 82 loss: 0.1459\n",
      "Batch 83 loss: 0.0891\n",
      "Batch 84 loss: 0.1034\n",
      "Batch 85 loss: 0.1216\n",
      "Batch 86 loss: 0.1098\n",
      "Batch 87 loss: 0.1429\n",
      "Batch 88 loss: 0.0932\n",
      "Batch 89 loss: 0.1117\n",
      "Batch 90 loss: 0.1156\n",
      "Batch 91 loss: 0.0774\n",
      "Batch 92 loss: 0.0874\n",
      "Batch 93 loss: 0.0984\n",
      "Batch 94 loss: 0.1441\n",
      "Batch 95 loss: 0.1126\n",
      "Batch 96 loss: 0.1150\n",
      "Batch 97 loss: 0.1280\n",
      "Batch 98 loss: 0.1163\n",
      "Batch 99 loss: 0.0767\n",
      "Batch 100 loss: 0.0848\n",
      "Batch 101 loss: 0.0879\n",
      "Batch 102 loss: 0.0975\n",
      "Batch 103 loss: 0.1072\n",
      "Batch 104 loss: 0.1085\n",
      "Batch 105 loss: 0.1083\n",
      "Batch 106 loss: 0.1109\n",
      "Batch 107 loss: 0.0929\n",
      "Batch 108 loss: 0.0641\n",
      "Batch 109 loss: 0.1026\n",
      "Batch 110 loss: 0.1123\n",
      "Batch 111 loss: 0.0760\n",
      "Batch 112 loss: 0.1206\n",
      "Batch 113 loss: 0.0980\n",
      "Batch 114 loss: 0.0902\n",
      "Batch 115 loss: 0.0889\n",
      "Batch 116 loss: 0.1451\n",
      "Batch 117 loss: 0.1269\n",
      "Batch 118 loss: 0.1103\n",
      "Batch 119 loss: 0.0918\n",
      "Batch 120 loss: 0.0785\n",
      "Batch 121 loss: 0.0816\n",
      "Batch 122 loss: 0.0947\n",
      "Batch 123 loss: 0.1082\n",
      "Batch 124 loss: 0.0472\n",
      "Batch 125 loss: 0.1368\n",
      "Batch 126 loss: 0.0805\n",
      "Batch 127 loss: 0.0929\n",
      "Batch 128 loss: 0.1200\n",
      "Batch 129 loss: 0.1308\n",
      "Batch 130 loss: 0.0862\n",
      "Batch 131 loss: 0.0753\n",
      "Batch 132 loss: 0.1040\n",
      "Batch 133 loss: 0.1168\n",
      "Batch 134 loss: 0.1174\n",
      "Batch 135 loss: 0.1083\n",
      "Batch 136 loss: 0.1141\n",
      "Batch 137 loss: 0.1387\n",
      "Batch 138 loss: 0.1006\n",
      "Batch 139 loss: 0.1085\n",
      "Batch 140 loss: 0.0885\n",
      "Batch 141 loss: 0.0967\n",
      "Batch 142 loss: 0.0999\n",
      "Batch 143 loss: 0.1071\n",
      "Batch 144 loss: 0.0954\n",
      "Batch 145 loss: 0.0884\n",
      "Batch 146 loss: 0.0764\n",
      "Batch 147 loss: 0.1337\n",
      "Batch 148 loss: 0.0775\n",
      "Batch 149 loss: 0.1020\n",
      "Batch 150 loss: 0.0881\n",
      "Batch 151 loss: 0.1463\n",
      "Batch 152 loss: 0.1013\n",
      "Batch 153 loss: 0.0988\n",
      "Batch 154 loss: 0.1256\n",
      "Batch 155 loss: 0.0742\n",
      "Batch 156 loss: 0.0928\n",
      "Batch 157 loss: 0.0983\n",
      "Batch 158 loss: 0.1011\n",
      "Batch 159 loss: 0.0888\n",
      "Batch 160 loss: 0.1103\n",
      "Batch 161 loss: 0.1075\n",
      "Batch 162 loss: 0.0846\n",
      "Batch 163 loss: 0.1116\n",
      "Batch 164 loss: 0.0908\n",
      "Batch 165 loss: 0.0654\n",
      "Batch 166 loss: 0.1291\n",
      "Batch 167 loss: 0.0928\n",
      "Epoch complete. Average loss: 0.1031\n",
      "Training Loss: 0.1031\n",
      "Validation Loss: 0.1217\n",
      "ROUGE-L: 0.4172\n",
      "BLEU: 0.2757\n",
      "\n",
      "Sample Results:\n",
      "Example 1:\n",
      "Text: 'menu testing with all of the duck ! this is the hard part . # toughwork  # menu # brewpub # openingsoon'\n",
      "Generated: the author is happy with the menu testing with all of the duck.\n",
      "Reference: menu testing with all of the duck isn't the hard part, it's fun.\n",
      "\n",
      "Example 2:\n",
      "Text: 'oh great , sky lanterns are sold for all seasons and holidays now ! i just love picking up these ! <user>  # skytrash'\n",
      "Generated: the author is pissed at <user> since the sky lanterns are sold for all seasons and holidays now.\n",
      "Reference: the author hates that sky lanterns are sold for all seasons and holidays now, it's inconvenient picking them up.\n",
      "\n",
      "Example 3:\n",
      "Text: 'so weather conditions are affecting <user> services . it does look dreadful outside ! ! ! # redhillmoo '\n",
      "Generated: the author doesn't think the weather conditions are affecting <user> services.\n",
      "Reference: <user> says weather conditions are affecting their services, but outside looks pretty pleasant to author.\n",
      "\n",
      "Example 4:\n",
      "Text: '<user> <user> nothing says fun like getting ready for a raid at 5:30 in the morning . '\n",
      "Generated: the author is pissed at <user> for getting ready for a raid at 5:30 in the morning.\n",
      "Reference: it's not fun getting ready for a raid at 5:30 in the morning.\n",
      "\n",
      "Example 5:\n",
      "Text: 'yeah $ 226 is great ! now i can buy the new # lebrons with all my # profit ... because '\n",
      "Generated: the author doesn't think $ 226 is great since he can't buy the new lebrons with all his profit.\n",
      "Reference: the author isn't satisfied with $ 226 since he can't buy the new lebrons with all his profit.\n",
      "\n",
      "Checkpoint saved to checkpoints\\muse_model_epoch_3.pt\n",
      "Epoch 4/20\n",
      "Training on epoch 4...\n",
      "Total number of batches: 168\n",
      "Batch 0 loss: 0.0849\n",
      "Batch 1 loss: 0.0957\n",
      "Batch 2 loss: 0.1040\n",
      "Batch 3 loss: 0.0531\n",
      "Batch 4 loss: 0.0963\n",
      "Batch 5 loss: 0.0651\n",
      "Batch 6 loss: 0.0829\n",
      "Batch 7 loss: 0.0912\n",
      "Batch 8 loss: 0.0751\n",
      "Batch 9 loss: 0.0779\n",
      "Batch 10 loss: 0.0968\n",
      "Batch 11 loss: 0.1023\n",
      "Batch 12 loss: 0.0934\n",
      "Batch 13 loss: 0.0963\n",
      "Batch 14 loss: 0.0715\n",
      "Batch 15 loss: 0.1016\n",
      "Batch 16 loss: 0.0676\n",
      "Batch 17 loss: 0.0778\n",
      "Batch 18 loss: 0.0946\n",
      "Batch 19 loss: 0.0598\n",
      "Batch 20 loss: 0.0712\n",
      "Batch 21 loss: 0.1009\n",
      "Batch 22 loss: 0.0752\n",
      "Batch 23 loss: 0.0731\n",
      "Batch 24 loss: 0.1328\n",
      "Batch 25 loss: 0.0783\n",
      "Batch 26 loss: 0.0674\n",
      "Batch 27 loss: 0.0970\n",
      "Batch 28 loss: 0.0773\n",
      "Batch 29 loss: 0.0860\n",
      "Batch 30 loss: 0.0804\n",
      "Batch 31 loss: 0.0856\n",
      "Batch 32 loss: 0.0866\n",
      "Batch 33 loss: 0.0763\n",
      "Batch 34 loss: 0.0713\n",
      "Batch 35 loss: 0.0678\n",
      "Batch 36 loss: 0.0620\n",
      "Batch 37 loss: 0.0709\n",
      "Batch 38 loss: 0.0744\n",
      "Batch 39 loss: 0.0659\n",
      "Batch 40 loss: 0.0816\n",
      "Batch 41 loss: 0.0683\n",
      "Batch 42 loss: 0.1058\n",
      "Batch 43 loss: 0.0622\n",
      "Batch 44 loss: 0.0866\n",
      "Batch 45 loss: 0.1129\n",
      "Batch 46 loss: 0.0910\n",
      "Batch 47 loss: 0.0850\n",
      "Batch 48 loss: 0.1005\n",
      "Batch 49 loss: 0.0902\n",
      "Batch 50 loss: 0.0872\n",
      "Batch 51 loss: 0.0738\n",
      "Batch 52 loss: 0.0858\n",
      "Batch 53 loss: 0.0825\n",
      "Batch 54 loss: 0.1131\n",
      "Batch 55 loss: 0.0811\n",
      "Batch 56 loss: 0.0763\n",
      "Batch 57 loss: 0.0848\n",
      "Batch 58 loss: 0.0769\n",
      "Batch 59 loss: 0.0878\n",
      "Batch 60 loss: 0.0710\n",
      "Batch 61 loss: 0.0489\n",
      "Batch 62 loss: 0.0695\n",
      "Batch 63 loss: 0.0687\n",
      "Batch 64 loss: 0.0760\n",
      "Batch 65 loss: 0.0698\n",
      "Batch 66 loss: 0.1064\n",
      "Batch 67 loss: 0.0680\n",
      "Batch 68 loss: 0.0707\n",
      "Batch 69 loss: 0.0821\n",
      "Batch 70 loss: 0.1213\n",
      "Batch 71 loss: 0.0721\n",
      "Batch 72 loss: 0.0995\n",
      "Batch 73 loss: 0.0903\n",
      "Batch 74 loss: 0.0735\n",
      "Batch 75 loss: 0.0665\n",
      "Batch 76 loss: 0.0696\n",
      "Batch 77 loss: 0.0855\n",
      "Batch 78 loss: 0.0733\n",
      "Batch 79 loss: 0.0812\n",
      "Batch 80 loss: 0.0654\n",
      "Batch 81 loss: 0.0850\n",
      "Batch 82 loss: 0.0810\n",
      "Batch 83 loss: 0.1254\n",
      "Batch 84 loss: 0.0894\n",
      "Batch 85 loss: 0.0937\n",
      "Batch 86 loss: 0.0738\n",
      "Batch 87 loss: 0.0691\n",
      "Batch 88 loss: 0.0915\n",
      "Batch 89 loss: 0.1045\n",
      "Batch 90 loss: 0.0757\n",
      "Batch 91 loss: 0.0987\n",
      "Batch 92 loss: 0.0719\n",
      "Batch 93 loss: 0.0961\n",
      "Batch 94 loss: 0.0819\n",
      "Batch 95 loss: 0.1029\n",
      "Batch 96 loss: 0.0766\n",
      "Batch 97 loss: 0.1253\n",
      "Batch 98 loss: 0.0813\n",
      "Batch 99 loss: 0.1107\n",
      "Batch 100 loss: 0.0788\n",
      "Batch 101 loss: 0.0705\n",
      "Batch 102 loss: 0.0926\n",
      "Batch 103 loss: 0.0675\n",
      "Batch 104 loss: 0.0787\n",
      "Batch 105 loss: 0.0941\n",
      "Batch 106 loss: 0.0861\n",
      "Batch 107 loss: 0.1090\n",
      "Batch 108 loss: 0.0685\n",
      "Batch 109 loss: 0.0677\n",
      "Batch 110 loss: 0.1015\n",
      "Batch 111 loss: 0.0614\n",
      "Batch 112 loss: 0.0632\n",
      "Batch 113 loss: 0.0855\n",
      "Batch 114 loss: 0.0685\n",
      "Batch 115 loss: 0.0659\n",
      "Batch 116 loss: 0.0792\n",
      "Batch 117 loss: 0.0648\n",
      "Batch 118 loss: 0.0749\n",
      "Batch 119 loss: 0.0652\n",
      "Batch 120 loss: 0.1033\n",
      "Batch 121 loss: 0.0885\n",
      "Batch 122 loss: 0.0763\n",
      "Batch 123 loss: 0.0827\n",
      "Batch 124 loss: 0.0731\n",
      "Batch 125 loss: 0.0790\n",
      "Batch 126 loss: 0.0647\n",
      "Batch 127 loss: 0.0756\n",
      "Batch 128 loss: 0.0719\n",
      "Batch 129 loss: 0.1027\n",
      "Batch 130 loss: 0.0590\n",
      "Batch 131 loss: 0.1159\n",
      "Batch 132 loss: 0.1062\n",
      "Batch 133 loss: 0.0956\n",
      "Batch 134 loss: 0.1072\n",
      "Batch 135 loss: 0.1099\n",
      "Batch 136 loss: 0.0977\n",
      "Batch 137 loss: 0.0900\n",
      "Batch 138 loss: 0.1010\n",
      "Batch 139 loss: 0.0891\n",
      "Batch 140 loss: 0.0757\n",
      "Batch 141 loss: 0.0971\n",
      "Batch 142 loss: 0.0701\n",
      "Batch 143 loss: 0.0922\n",
      "Batch 144 loss: 0.1057\n",
      "Batch 145 loss: 0.0854\n",
      "Batch 146 loss: 0.0715\n",
      "Batch 147 loss: 0.0683\n",
      "Batch 148 loss: 0.0833\n",
      "Batch 149 loss: 0.0906\n",
      "Batch 150 loss: 0.0739\n",
      "Batch 151 loss: 0.0724\n",
      "Batch 152 loss: 0.0880\n",
      "Batch 153 loss: 0.0852\n",
      "Batch 154 loss: 0.0767\n",
      "Batch 155 loss: 0.0965\n",
      "Batch 156 loss: 0.0899\n",
      "Batch 157 loss: 0.0719\n",
      "Batch 158 loss: 0.0751\n",
      "Batch 159 loss: 0.0716\n",
      "Batch 160 loss: 0.0900\n",
      "Batch 161 loss: 0.0756\n",
      "Batch 162 loss: 0.0886\n",
      "Batch 163 loss: 0.1224\n",
      "Batch 164 loss: 0.0850\n",
      "Batch 165 loss: 0.1151\n",
      "Batch 166 loss: 0.0736\n",
      "Batch 167 loss: 0.0679\n",
      "Epoch complete. Average loss: 0.0838\n",
      "Training Loss: 0.0838\n",
      "Validation Loss: 0.1214\n",
      "ROUGE-L: 0.4159\n",
      "BLEU: 0.2786\n",
      "\n",
      "Sample Results:\n",
      "Example 1:\n",
      "Text: 'menu testing with all of the duck ! this is the hard part . # toughwork  # menu # brewpub # openingsoon'\n",
      "Generated: the author is excited for the menu testing with all of the duck.\n",
      "Reference: menu testing with all of the duck isn't the hard part, it's fun.\n",
      "\n",
      "Example 2:\n",
      "Text: 'oh great , sky lanterns are sold for all seasons and holidays now ! i just love picking up these ! <user>  # skytrash'\n",
      "Generated: the author is disappointed with the author since sky lanterns are sold for all seasons and holidays.\n",
      "Reference: the author hates that sky lanterns are sold for all seasons and holidays now, it's inconvenient picking them up.\n",
      "\n",
      "Example 3:\n",
      "Text: 'so weather conditions are affecting <user> services . it does look dreadful outside ! ! ! # redhillmoo '\n",
      "Generated: the author is pissed at <user> for such bad weather.\n",
      "Reference: <user> says weather conditions are affecting their services, but outside looks pretty pleasant to author.\n",
      "\n",
      "Example 4:\n",
      "Text: '<user> <user> nothing says fun like getting ready for a raid at 5:30 in the morning . '\n",
      "Generated: nobody likes getting ready for a raid at 5:30 in the morning.\n",
      "Reference: it's not fun getting ready for a raid at 5:30 in the morning.\n",
      "\n",
      "Example 5:\n",
      "Text: 'yeah $ 226 is great ! now i can buy the new # lebrons with all my # profit ... because '\n",
      "Generated: the author doesn't think $ 226 is great since he can't buy the new lebrons with all his profit.\n",
      "Reference: the author isn't satisfied with $ 226 since he can't buy the new lebrons with all his profit.\n",
      "\n",
      "Checkpoint saved to checkpoints\\muse_model_epoch_4.pt\n",
      "Epoch 5/20\n",
      "Training on epoch 5...\n",
      "Total number of batches: 168\n",
      "Batch 0 loss: 0.0843\n",
      "Batch 1 loss: 0.0699\n",
      "Batch 2 loss: 0.0540\n",
      "Batch 3 loss: 0.0743\n",
      "Batch 4 loss: 0.0630\n",
      "Batch 5 loss: 0.0715\n",
      "Batch 6 loss: 0.0738\n",
      "Batch 7 loss: 0.0679\n",
      "Batch 8 loss: 0.0502\n",
      "Error in batch 8: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model, train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m MuSEModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving final model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 152\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, tokenizer, device, epochs, lr, checkpoint_dir)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# train - 1 epoch\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 47\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     49\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch complete. Average loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 40\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     43\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "model, train_results = train_and_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
